\chapter{Clustering}
Il \textbf{clustering} è un approccio \textit{non supervisionato}, ovvero il
sistema è in grado di apprendere informazioni dai dati senza che gli vengano
fornite le etichette. Il sistema deve essere in grado di raggruppare gli elementi
in base a delle caratteristiche comuni.

Il sistema è quindi flessibile nel decidere i criteri con cui raggruppare i dati,
questo non può essere scelto in base alle label, ma deve essere scelto ad esempio
sfruttando concetti sulla la distribuzione dei dati. I risultati ottenuti da un
apprendimento non supervisionato potrebbero “sorprendere” in quanto potrebbero essere
imprevedibili a priori. In ogni caso l'incertezza del clustering non è la sola,
in quanto anche un sistema supervisionato potrebbe comunque lasciare spazio ad
incertezza sulle ipotesi.

Avendo una classificazione non supervisionata si ha una scarsa conoscenza a
priori dei dati da analizzare, puntando quindi all'estrazione automatica delle
classi, mentre in quella supervisionata si partiva da classi etichettate, avendo
magari speso costo computazionale per ottenerle, e da una struttura classificatoria
conosciuta. I sistemi non supervisionati tornano comodi quando la distribuzione dei
valori degli attributi è informazione sufficiente per separare le istanze in più
classi, si hanno quindi domini dove questo fattore è determinante. Con
l'apprendimento non supervisionato quindi la non conoscenza a priori è anche un
vantaggio (magari le etichette che uso nel supervisionato potrebbero essere errate),
insieme alla riduzione dell'errore umano. Un altro pro è che tutte le classi con
caratteristiche uniche vengono identificati e quindi i metodi di apprendimento
non supervisionato sono efficaci con elementi di tipo numerico, che diventano
coordinate in uno spazio, e con elementi dotati di ordinamento intrinseco, per
il calcolo delle distanze. Di contro le classi ottenute non per forza hanno un
significato. Inoltre l'utente ha poco controllo sulla procedura e sui risultati e
tali metodi sono meno efficaci con elementi ordinati in modo arbitrario o parziale.

I dati che vengono utilizzati per effettuare il clustering sono principalmente
vettori numerici, dove ogni elemento misura una specifica \textbf{caratteristica}
(\textbf{feature}) dell'istanza. Per poter raggruppare questi elementi è necessario
definire dei criteri di similarità tra i vettori.

Si hanno quindi due criteri da rispettare:
\begin{itemize}
    \item \textbf{Omogeneità}: gli elementi di un cluster devono essere simili tra loro.
    \item \textbf{Separazione}: gli elementi di cluster diversi devono essere dissimili tra loro.
\end{itemize}
\begin{definizione}[\textbf{Cluster}]
    Sia $N = \{e_1, \dots, e_n\}$ l'insieme degli elementi da raggruppare e sia
    $C = \{c_1, \dots, c_k\}$ una partizione di $N$ in sottoinsiemi disgiunti
    $C_i$. Ogni sottoinsieme $C_i$ è un \textbf{cluster}. Due elementi si chiamano
    \textbf{mates} rispetto a $C$ se sono membri dello stesso cluster.
\end{definizione}
\begin{definizione}[\textbf{Misura di similarità}]
    Possiamo definire una \textbf{misura di similarità} tra due elementi $e_i$ e
    $e_j$ utilizzando la distanza. In particolare possiamo utilizzare:
    \begin{itemize}
        \item \textbf{Distanza euclidea}: invariante rispetto a traslazioni e
              rotazioni degli assi, rappresenta la lunghezza del segmento tra i due punti.
              \begin{equation}
                  d(e_i, e_j) = \sqrt{\sum_{k=1}^n (e_{ik} - e_{jk})^2}
              \end{equation}
        \item \textbf{Distanza di Manhattan}: non è invariante rispetto a traslazioni
              o rotazioni degli assi e pone meno enfasi sulle variabili con distanze maggiori,
              non elevando al quadrato le differenze.
              \begin{equation}
                  d(e_i, e_j) = \sum_{k=1}^n |e_{ik} - e_{jk}|
              \end{equation}
        \item \textbf{Distanza di Minkowski}:
              \begin{equation}
                  d(e_i, e_j) = \sqrt[p]{\sum_{k=1}^n |e_{ik} - e_{jk}|^p}
              \end{equation}
              La distanza di Manhattan è un caso particolare della distanza di
              Minkowski con $p=1$. La distanza euclidea è un caso particolare della
              distanza di Minkowski con $p=2$. La distanza di Minkowski è un caso
              particolare della distanza di Minkowski con $p=\infty$.
    \end{itemize}
\end{definizione}
Esistono diverse tipi di clustering, possiamo principlamente inividuare:
\begin{itemize}
    \item \textbf{Clustering gerarchico}: si costruisce una gerarchia di cluster.
          Si parte da un cluster che contiene tutti gli elementi e si procede a dividere
          i cluster in sotto-cluster. Si può procedere in modo top-down o bottom-up. Si
          collocano gli elementi in input in una struttura gerarchica ad albero, in cui
          le distanze tra nodi riflettono le similarità degli elementi. Gli
          elementi sono localizzati sulle foglie dell'albero.
    \item \textbf{Clustering partizionale}: si costruisce una partizione di $N$
          in $k$ cluster. Si parte da una partizione iniziale e si procede a migliorarla
          iterativamente. Si può procedere in modo deterministico o probabilistico.
          Si collocano gli elementi in input in un insieme di cluster, in cui le
          distanze tra i cluster riflettono le similarità degli elementi. Gli elementi
          sono localizzati nei cluster. I metodi non gerarchici mirano a ripartire le
          $n$ unità della popolazione in $k$ gruppi, fornendo una sola partizione anziché
          una successione di partizioni tipica dei metodi gerarchici
\end{itemize}
Esistono algoritmi di clustering che funzionano su grafi, in questo caso la distanza
è data dall'arco. Noi ci concentriamo su algoritmi che funzionano su vettori di
numeri reali.
\section{K-Means}
\begin{definizione}[\textbf{Dendrogramma}]
    Un \textbf{dendrogramma} è un metodo per rappresentare le informazioni come
    un albero o un grafo utilizzato per visualizzare la somiglianza nel processo
    di raggruppamento.
\end{definizione}
L'algoritmo $k-$means ha come prerequisito quello di conoscere il numero di cluster
che si vogliono andare ad individuare. L'obiettivo di questo algoritmo è quello
di minimizzare le distanze tra gli elementi di un cluster e il centroide dello stesso.
Spostando i centroidi si ottiene una nuova partizione.
\begin{definizione}[\textbf{Centroide}]
    Nel caso dell'algoritmo $k-$means il \textbf{centroide} è ottenuto come media
    di tutti i punti apparteneti al cluster.
\end{definizione}
L'algoritmo del $k-$means lavora con dati numerici nel seguente modo:
\begin{enumerate}
    \item Si fissano a caso $k$ centroidi iniziali di altrettanti cluster.
    \item Per ogni individuo si calcola la distanza da ciascun centroide e lo si
          assegna al più vicino.
    \item Per la partizione provvisoria così ottenuta si ricalcolano i centroidi
          di ogni cluster (\textit{media aritmetica}).
    \item Per ogni individuo si ricalcola la distanza dai centroidi e si effettuano
          gli eventuali spostamenti tra cluster
    \item Si ripetono le operazioni 3 e 4 finché si raggiunge il numero massimo
          di iterazioni impostate o non si verificano altri spostamenti.
\end{enumerate}
Questo algoritmo è molto semplice da implementare e richiede un tempo di calcolo
pari a:$\mathcal{O}(tKn)$ con:
\begin{itemize}
    \item $n$ cardinalità dell'insieme dei dati.
    \item $K$ numero di cluster.
    \item $t$ numero di iterazioni del ciclo (avendo quindi $kt << n$)
\end{itemize}
Di contro si ha una sensibilità rispetto alla scelta dei centroidi iniziali,
sperimentalmente si è dimostrato che brutti valori iniziali invalidano l'intero
processo. Inoltre, come detto all'inizio, non si può predire il numero di cluster
non conoscendo a priori i dati e non esiste un $K$ ottimale e non ci sono proprietà
che ce lo possano suggerire.

In ogni caso mappa sensatamente i vari elementi in base alle loro caratteristiche
e quindi è un approccio parecchio usato, essendo generalmente efficacie.

Non sapendo a priori il numero di $K$ posso ottenere scarsi risultati magari non
avendo abbastanza centroidi. Adattarsi ad un numero “errato” di centroidi può
portare a risultati “sporchi”.

Un altro problema è dato da dati distribuiti secondo diverse “dimensioni” nello
spazio ma l'algoritmo lavorando sui centroidi potrebbe classificare in modo errato
questo dettaglio, non distinguendo i corretti insiemi di punti. Analogamente
succede se si hanno insiemi di punti più o meno densi, portando a classificazioni errate.

Sempre in modo analogo le proprietà di distribuzione geometrica degli elementi
comporta problemi (anche con un K adeguato). Per superare queste difficoltà si
può provare ad aumentare il numero di cluster, unendo poi, in un secondo passaggio,
i vari cluster secondo certi criteri, magari avendo una netta separazione lineare tra cluster.

Per misurare le prestazioni di clustering bisogna capire come classificare i
risultati. Si usa la misura di \textbf{silhouette} per capire se un clustering è
migliore di un altro. Tale misura si basa sempre sul concetto di distanza.

Si ha una tecnica di misura, supponendo che ogni cluster abbia almeno due elementi,
avendo tecniche diverse qualora si abbia cardinalità 1, avendo la distanza tra
due elementi $d(i, j)$ e due concetti:
\begin{enumerate}
    \item Distanza media \textbf{intra-cluster}, ovvero dentro i cluster:
          \begin{equation}
              a(i) = \frac{1}{|C_i| - 1} \sum_{j \in C_i, j \neq i} d(i, j)
          \end{equation}
    \item Distanza media \textbf{inter-cluster}, ovvero tra cluster diversi:
          \begin{equation}
              b(i) = \min_{k \neq i} \frac{1}{|C_k|} \sum_{j \in C_k} d(i, j)
          \end{equation}
\end{enumerate}
ottenendo quindi la silhouette per il punto i come:
\begin{equation}
    s(i) = \frac{b(i) - a(i)}{\max\{a(i), b(i)\}}
\end{equation}
La qualità viene quindi visualizzata da un diagramma che studia la silhouette al
variare di $K$ per ogni valore di ogni attributo (mettendo per ogni attributo in
alto i valori più studiabili).

La media $s(i)$ su tutti i punti di un cluster è una misura di quanto siano
strettamente raggruppati tutti i punti del cluster. Pertanto, la media $s(i)$ su
tutti i dati dell'intero set di dati è una misura di quanto adeguatamente i dati
sono stati raggruppati. Se ci sono troppi o troppo pochi cluster, come può accadere
quando una scelta sbagliata di $k$ viene utilizzata nell'algoritmo di clustering,
alcuni dei cluster mostreranno tipicamente linee molto più strette rispetto al
resto nel diagramma di silhouette. Quindi grafici e media di silhouette possono
essere utilizzati per determinare il numero “naturale” di cluster all'interno di
un set di dati.