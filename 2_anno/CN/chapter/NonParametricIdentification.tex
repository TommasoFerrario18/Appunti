\chapter{Non-parametric Identification}
\section{Frontdoor Adjustment}
We have seen that the \textbf{backdoor criterion} is sufficient for identification
but not necessary. In other words, is it possible to get identifiability without
being able to block all backdoor paths? Also, if we have unobserved variables we
\textbf{cannot} block the backdoor path.

If we are only focusing on the data we can have multiple interpretations of them.
So we need a method that we can use for causal estimation.

This can be obtained by chaining together the partial effects to obtain the overall effect.
\begin{definition}[\textbf{Frontdoor Criterion}]
    A set of variables $\mathbf{S}$ is said to satisfy the front-door criterion
    relative to an ordered pair of variables $(X, Y)$ if:
    \begin{enumerate}
        \item $\mathbf{S}$ intercepts all directed paths from $X$ to $Y$;
        \item There is no unblocked backdoor path from $X$ to $\mathbf{S}$;
        \item All backdoor paths from $\mathbf{S}$ to $Y$ are blocked by $X$.
    \end{enumerate}
\end{definition}
The conditions are overly conservative; some of the backdoor paths excluded by
conditions (2) and (3) can be allowed provided they are blocked by some
variables.

There is a powerful symbolic machinery, called the \textbf{do-Calculus}, that allows
analysis of such intricate structures. The do-Calculus uncovers all causal
effects that can be identified from a given graph.

\begin{definition}[\textbf{Frontdoor Adjustment}]
    If $\mathbf{S}$ satisfies the \textbf{frontdoor criterion} relative to $(X, Y)$
    and if $P(x, s) > 0$, then the causal effect of $X$ on $Y$ is identifiable
    and is given by the formula:
    \begin{equation}
        P(y | do(x)) = \sum_{\mathbf{s}} P(\mathbf{s} | x) \sum_{x'} P(y | \mathbf{s}, x') \cdot P(x')
    \end{equation}
\end{definition}

Satisfying the backdoor criterion isn't necessary to identify causal effects. If
the front-door criterion is satisfied, that also gives us \textit{identifiability}.

The combination of the adjustment formula, the backdoor criterion, and the frontdoor
criterion covers numerous scenarios. It proves the enormous, even revelatory, power
that causal graphs have in not merely representing but discovering causal
information.
\section{Do-calculus}
We can \textbf{identify} a causal query even if the causal graph doesn't satisfy
backdoor and frontdoor criterion. We can use \textbf{do-calculus} to
identify any identifiable causal effect where there are multiple treatments and/or
multiple outcomes.

To introduce and discuss do-calculus we first need to give some more notation:
\begin{itemize}
    \item $\perp_\mathcal{G}$ is the d-separation in the causal graph $\mathcal{G}$;
    \item $\mathcal{G}_{\overline{X}}$ is the graph where all the incoming edges
          in $X$ have been removed.
    \item $\mathcal{G}_{\underline{X}}$ is the graph where all the outgoing edges
          from $X$ have been removed.
    \item $\mathcal{G}_{\overline{Y}\underline{X}}$ is the graph where all the
          incoming edges in $Y$ and all the outgoing edges from $X$ have been removed.
\end{itemize}
\begin{definition}[\textbf{Rules of do-calculus}]
    Given a causal graph $\mathcal{G}$, an associated distribution $P$, and
    disjoint set of variables $\mathbf{Y}, \mathbf{X}, \mathbf{Z}$ and $\mathbf{W}$,
    the following rules hold:
    \begin{itemize}
        \item \textbf{Rule 1}:
              \begin{equation}
                  P(\mathbf{y} | do(\mathbf{x}), \mathbf{z}, \mathbf{w}) =
                  P(\mathbf{y} | do(\mathbf{x}), \mathbf{w}) \, \text{ if } \, \mathbf{Y}
                  \perp_{\mathcal{G}_{\overline{\mathbf{X}}}} \mathbf{Z} | \mathbf{X}, \mathbf{W}
              \end{equation}
              This is true because if we remove $do(x)$ we obtain:
              \begin{equation*}
                  P(\mathbf{y} |\mathbf{z}, \mathbf{w}) = P(\mathbf{y} | \mathbf{w}) \, \text{ if } \, \mathbf{Y}
                  \perp_{\mathcal{G}_{\overline{\mathbf{X}}}} \mathbf{Z} | \mathbf{W}
              \end{equation*}
              We can see that is a d-separation under the global Markov assumption,
              because d-separation in the graph $\mathcal{G}$ implies conditional
              independence in $P$ between $\mathbf{Y}$ and $\mathbf{Z}$ giving
              $\mathbf{W}$, so the separating set is $\mathbf{W}$.

              This means that Rule 1 is simply a generalization of d-separation to
              interventional distributions;
        \item \textbf{Rule 2}:
              \begin{equation}
                  P(\mathbf{y} | do(\mathbf{x}), do(\mathbf{z}), \mathbf{w}) =
                  P(\mathbf{y} | do(\mathbf{x}), \mathbf{z}, \mathbf{w}) \,
                  \text{ if } \, \mathbf{Y} \perp_{\mathcal{G}_{\overline{\mathbf{X}}\underline{\mathbf{Z}}}} \mathbf{Z} | \mathbf{X}, \mathbf{W}
              \end{equation}
              Also in this case we can remove $do(x)$ and we obtain:
              \begin{equation*}
                  P(\mathbf{y} | do(\mathbf{z}), \mathbf{w}) = P(\mathbf{y} | \mathbf{z}, \mathbf{w}) \,
                  \text{ if } \, \mathbf{Y} \perp_{\mathcal{G}_{\overline{\mathbf{Z}}}} \mathbf{Z} | \mathbf{W}
              \end{equation*}
              This corresponds to apply the backdoor adjustment using the backdoor
              criterion. So the rule is the generalization of backdoor adjustment to
              ana interventional distribution;
        \item \textbf{Rule 3}:
              \begin{equation}
                  P(\mathbf{y} | do(\mathbf{x}), do(\mathbf{z}), \mathbf{w}) =
                  P(\mathbf{y} | do(\mathbf{x}), \mathbf{w}) \, \text{ if } \,
                  \mathbf{Y} \perp_{\mathcal{G}_{\overline{\mathbf{X}}\overline{\mathbf{Z}(\mathbf{W})}}} \mathbf{Z} | \mathbf{X}, \mathbf{W}
              \end{equation}
              where $\overline{\mathbf{Z}(\mathbf{W})}$ denotes the set of nodes
              of $\mathbf{Z}$ that aren't ancestor of any node of $\mathbf{W}$
              in $\mathcal{G}_{\overline{\mathbf{X}}}$.
    \end{itemize}
\end{definition}
We could ask whether there could exist causal estimands that are identifiable but
that can't be identified using only the rules of do-calculus. Fortunately, it
has been proved that do-calculus is \textbf{complete}, which means that these
three rules are sufficient to identify all identifiable causal estimands. Because
these proofs are constructive, they also admit algorithms that identify any causal
estimand in polynomial time.

Do-calculus tells us if we can identify a given causal estimand using only the
causal assumptions encoded in the causal graph. If we introduce more assumptions
about the distribution ex: linearity, we can identify more causal estimands, but
this is known as \textbf{parametric identification}.

\section{Determining identifiability from the graph}
We previously mentioned that do-calculus is complete, which means that three rules
are sufficient to identify all identifiable causal estimands, also that estimands
that aren't identifiable by just using the causal graph. However, it would be
much more satisfying to know whether a causal estimand is identifiable by simply
looking at the causal graph.

For example, the backdoor criterion and the frontdoor criterion gave us simple
ways to know for sure that a causal estimand is identifiable. However, plenty of
causal estimands are identifiable, even though the corresponding
causal graphs don't satisfy the backdoor or frontdoor criterion.

Tian and Pearl provide a relatively simple graphical criterion that is sufficient
for identifiability: the \textbf{Unconfounded Children Criterion}.

\begin{definition}[\textbf{Unconfounded Children Criterion}]
    This criterion is satisfied if it is possible to block all backdoor paths
    from the treatment variable $X$ to all of its children ($ch(X)$) that are
    ancestors ($an(Y)$) of $\mathbf{Y}$ with a \textbf{single} conditioning set $\mathbf{S}$.
\end{definition}

This criterion generalizes the backdoor criterion and the frontdoor criterion.
Like the backdoor criterion and the frontdoor criterion, \textbf{Unconfounded children
    criterion} is a sufficient condition for identifiability.

\begin{definition}[\textbf{Unconfounded Children Identifiability}]
    Let $\mathbf{Y}$ be the set of outcome variables and $X$ be a single variable.
    If the unconfounded children criterion and positivity are satisfied, then:
    \begin{equation}
        P(\mathbf{Y} = \mathbf{y} |do(x))
    \end{equation}
    is identifiable.
\end{definition}

If we can isolate all of the causal association flowing out of treatment $X$ along
directed paths to $\mathbf{Y}$, we have identifiability.
\begin{enumerate}
    \item First, consider that all of the causal associations from $X$ to $\mathbf{Y}$
          must flow through its children $ch(X)$.
    \item We can isolate this causal association if there is no confounding between
          $X$ and any of its children $ch(X)$.
    \item This isolation of all of the causal associations is what gives us
          identifiability of the causal effect of $X$ on any other node in the graph.
    \item This intuition might lead you to suspect that this criterion is necessary
          in the very specific case where the outcome set $\mathbf{Y}$ is all of
          the other variables in the graph other than $X$; it turns out that this
          is true. But this condition is not necessary if $\mathbf{Y}$ is a smaller
          set than that.
\end{enumerate}

In conclusion, the unconfounded children's identifiability is sufficient but not
necessary, and this related condition is necessary but not sufficient and works
on single variable $X, Y$.

Shpitser and Pearl provide a necessary and sufficient criterion for identifiability of:
\begin{equation}
    P(\mathbf{Y} = \mathbf{y}|do(\mathbf{X} = \mathbf{x}))
\end{equation}
when $\mathbf{Y}$ and $\mathbf{X}$ are arbitrary sets of variables: \textbf{the hedge criterion}.

Moving further along, Shpitser and Pearl provide a necessary and sufficient criterion
for the most general type of causal estimand: \textbf{conditional causal effects},
which takes the form:
\begin{equation}
    P(\mathbf{Y} = \mathbf{y}|do(\mathbf{X} = \mathbf{x}), \mathbf{Z} = \mathbf{z})
\end{equation}
where $\mathbf{Y}, \mathbf{X}$, and $\mathbf{Z}$ are all arbitrary sets of variables.