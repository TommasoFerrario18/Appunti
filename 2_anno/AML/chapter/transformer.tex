\chapter{Transformers}
\textbf{Transformer} were initially targeted at natural language processing (NLP) problems, where the 
network input is a series of high-dimensional embeddings representing words or word fragments. This
architecture was designed with the goal of processing this text into a representation suitable for
downstream tasks. 

We can also encounter different problems such as:
\begin{itemize}
    \item The encoded input can be very large. Assuming a 1024-d embedding and body of text that have 100s/1000s of
        words, a fully connected neural network is impractical because we have $100 \times 1024 = 102400$ as input
        length. 
    \item Each input is of different length.
    \item The language is fundamentally ambiguous.
\end{itemize}
\section{Dot-product self attention}
Considering the issues just presented, we want to design a network that is able to process text and can:
\begin{enumerate}
    \item Use parameter sharing to cope with long input passages of differing lengths.
    \item Contain connections between word representations that depend on the words themselves.
\end{enumerate}
The transformer acquires both properties by using \textbf{dot-product self-attention}. A self-attention block $sa[\cdot]$
takes $N$ inputs $x_n$, each of dimension $D \times 1$, and returns $N$ output vectors of the same size. Let's go into 
more detail. First, a set of \textbf{values} are computed for each input:
\begin{equation}
    v_n = \beta_v + \Omega_v \cdot x_n
\end{equation}
where $\beta_v$ contains the biases and $\Omega_v$ contains the weights. Then the $n$-th output $sa[x_n]$ is a weighted 
sum of all the values $v_n$:
\begin{equation}
    sa[x_n] = \sum_{m = 1}^N a[x_m, x_n] \cdot v_m
\end{equation}
The scalar weight $a[x_m, x_n]$ is the \textbf{attention} that input $x_n$ pays to input $x_m$. The $N$ weights $a[\cdot 
    , x_n]$ are non-negative and sum to one. 

Self-attention can be thought of as routing the values in different proportions to create each output.
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.3\linewidth]{img/transformer/selfattention.png}
    \caption{Self attention}
    \label{fig:values}
\end{figure}

To compute the values, the same weights $\Omega_v$ (with size $D \times D$) and biases $\beta_v$ (with size $D \times 1$)
are applied to each input input $x_n$ (with size $D \times 1$):
\begin{equation}
    v_n = \beta_v + \Omega_v \cdot x_n
\end{equation}
This computation scales linearly with the sequence length $N$, so it requires less parameters than a fully connected 
neural network connecting all $DN$ inputs to the $DN$ outputs. 

The value computation can thus be viewed as a sparse matrix multiplication with shared parameters.
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.5\linewidth]{img/transformer/values.png}
    \caption{Values matrix form}
    \label{fig:enter-label}
\end{figure}

The attention weights $a[x_m, x_n]$ combine the values from different inputs. They are also sparse, since there is only
one weight for each ordered pair of inputs $(x_m, x_n)$regardless of the size of these inputs.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\linewidth]{img/transformer/attention.png}
    \caption{Attention}
    \label{fig:enter-label}
\end{figure}

The number of attention weights has a quadratic dependence on the sequence length $N$ but it is independent of the length
$D$ of each input $x_n$.

Through this process, you get the same output as that from two-chained linear transformations:
\begin{itemize}
    \item The value vectors are computed independently from each input, 
    \item and these vectors are linearly combined by the attention weights.
\end{itemize}

However, the overall self-attention is nonlinear because the attention weights are themselves nonlinear functions of the 
input. This is an example of a \textbf{hypernetwork}, where one network branch computes the weights of another branch.

To compute the \textit{attention}, we apply two more linear transformations to the inputs: 
\begin{equation}
    q_n = \beta_q + \Omega_q \cdot x_n \,\,\, \land \,\,\, k_n = \beta_k + \Omega_k \cdot x_n
\end{equation}
where $q_n$ and $k_n$ are referred to as the \textbf{queries} and the \textbf{keys}. 

We then compute dot products between the queries and the keys followed by a softmax: 
\begin{equation}
    a[x_m, x_n] = softmax_m [k_\ast^T, q_n] = \frac{\exp[k_\ast^T, q_n]}{\sum_{m' = 1}^N \exp[k_\ast^T, q_n]}
\end{equation}
so, for each $x_n$ they are positive and sum to one.

The names \textit{queries} and \textit{keys} have the following interpretation:
\begin{itemize}
    \item The dot-product operation returns a measure of similarity between its inputs, so the weights $a[x_\ast, x_n]$ 
        depend on the relative similarities between each query and the keys.
    \item The softmax function means that we can think of the key vectors as \textit{competing} with one another to 
        contribute to the final result. 
\end{itemize}
The queries and keys must have the same dimensions. However, these can differ from the dimension of the values, which is 
usually the same size as the input, so that the representation does not change size.

This mechanism fulfills the initial requirements: 
\begin{enumerate}
    \item There is a single shared set of parameters $\phi = \{\beta_v, \Omega_v, \beta_q, \Omega_q, \beta_k, 
        \Omega_k\}$. This is independent of the number of inputs $N$, so the networks can be applied to difference 
        sequence lengths.
    \item There are connections between the inputs (words), and the strength of these connections depends on the inputs 
        themselves via the attention weights.
\end{enumerate}

This can be written in a compact form if the $N$ inputs $x_n$ form the columns of the $D \times N$ matrix $X$. The 
values, queries, and keys can be computed as:
\begin{align*}
        V[X] = \beta_v \cdot 1^T + \Omega_v \cdot X \\ 
        Q[X] = \beta_q \cdot 1^T + \Omega_q \cdot X \\
        K[X] = \beta_k \cdot 1^T + \Omega_k \cdot X
\end{align*}
The self-attention computation is then: 
\begin{equation}
    sa[X] = V[X] \cdot softmax[K[X]^T, Q[X]]
\end{equation}
where softmax is independently applied on the columns of its input.

Since the dot-product in the attention calculation can result in large magnitudes, it pushes the arguments of the softmax 
function into a region where the largest value dominates entirely. This causes small changes to the softmax inputs to 
have minimal impact on the output, making the model harder to train. 

To mitigate this, the dot products are scaled by the square root of the size of the query $D_q$ and key $D_k$, which 
corresponds to the number of rows in $\Omega_q$ and $\Omega_k$ as the query and key dimensions must be the same.
\begin{equation}
    sa[X] = V \cdot softmax\left[\frac{K^T \cdot Q]}{\sqrt{D_q}}\right]
\end{equation}

\section{Positional encoding}
The self-attention mechanism discards important information, in fact, the computation is the same regardless of the order 
of the inputs. 

However, the order is important when the inputs correspond to the words in a sentence. There are two main approaches to 
incorporating position information: 
\begin{itemize}
    \item Absolute position embeddings;
    \item Relative position embeddings.
\end{itemize}
\subsection{Absolute position embeddings}
This method consists of adding a $\Pi$ matrix to the $X$ input. The purpose of this matrix is to encode positional
information. Each column of $\Pi$ is unique and contains information about the absolute position in the input sequence. 

The matrix that is usually used for this purpose can be chosen by the architect or learned in the model training phase. 
Typically, this matrix is added to the model's input. Alternatively, since the outputs of the self-attention blocks have
the same dimensions as the input, the matrix can be added after each layer.

\begin{note}
    Sometimes it is added to $X$ in the computation of the queries and keys but not to the values.
\end{note}

\subsection{Relative position embeddings}
In this case, we are considering a situation where the input to a self-attention mechanism may be an entire sentence, 
many sentences, or just a fragment of a sentence, and the absolute position of a word is much less important than the 
relative position between two inputs.

Of course, this can be recovered if the system knows the absolute position of both, but relative position embeddings 
encode this information directly. Each element of the attention matrix corresponds to a particular offset between query 
position $a$ and key position $b$. 

Relative position embeddings learn a parameter $\pi_{a, b}$, for each offset and use this to modify the attention matrix 
by adding these values, multiplying by them, or using them to alter the attention matrix in some other way.
\section{Multiple heads self attention}
In the most used architectures, multiple self-attention mechanisms are used in parallel, and this is known as 
\textbf{multi-head self-attention}. If we now consider the fact of having $H$ self-attention blocks, we have that the 
different sets of values, keys and queries are calculated as: 
\begin{align*}
        V_h[X] = \beta_{vh} \cdot 1^T + \Omega_{vh} \cdot X \\ 
        Q_h[X] = \beta_{qh} \cdot 1^T + \Omega_{qh} \cdot X \\
        K_h[X] = \beta_{kh} \cdot 1^T + \Omega_{kh} \cdot X
\end{align*}
Then, the $h-$th self-attention mechanism or \textbf{head} can be written as:
\begin{equation}
    sa_h[X] = V_h \cdot softmax\left[\frac{K_h^T \cdot Q_h]}{\sqrt{D_q}}\right]
\end{equation}
where we have different parameters for each head. 

Typically, if the dimension of the inputs $x_m$ is $D$ and there are $H$ heads, the values, queries, and keys will all be 
of size $D/H$ as this allows for an efficient implementation. The outputs of these self-attention mechanisms are then 
vertically concatenated, and another linear transform $\Omega_c$ is applied to combine them: 
\begin{equation}
    Mhsa[X] = \Omega_c[sa_1[X]^T, sa_2[X]^T, \dots, sa_H[X]^T]
\end{equation}
This last transformation is useful because it allows to bring the weights of the different heads in the same range. In 
addition, if necessary, this combination can be used to limit a head that provides wrong data in output by setting it to 
zero.

Multiple heads seem to be necessary to make the transformer work well. It has been speculated that they make the self-
attention network more robust to bad initializations.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\linewidth]{img/transformer/multihead.png}
    \caption{Multihead}
    \label{fig:enter-label}
\end{figure}
\section{Transformer layers}
All the components we have defined so far serve as a basis for defining the transformer layer.

Transformer layer consists of a multi-head self-attention unit, which allows the word representations to interact with 
each other, followed by a fully connected network $mlp[x_\ast]$ that operates separately on each word. 

Both units are residual networks. In addition, it is typical to add a LayerNorm operation after both the self-attention 
and fully connected networks. In a real network, the data passes through a series of these layers.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\linewidth]{img/transformer/transformerlayers.png}
    \caption{Caption}
    \label{fig:enter-label}
\end{figure}

\section{NLP pipeline}
A typical NLP pipeline starts with a tokenizer that splits the text into a vocabulary of smaller constituent units 
(tokens) that the subsequent network can process. These tokens represent words, but there are several difficulties: 
\begin{itemize}
    \item Inevitably, some words (e.g., names) will not be in the vocabulary. 
    \item It's unclear how to handle punctuation, but this is important. If a sentence ends in a question mark, we must 
        encode this information. 
    \item The vocabulary would need different tokens for versions of the same word with different suffixes and there is
        no way to clarify that these variations are related. 
\end{itemize}

Then each of these tokens is mapped to a learned embedding. These embeddings are passed through a series of transformer 
layers. We now consider each of these stages in turn.

In practice, a compromise between letters and full words is used, and the final vocabulary includes both common words and 
word fragments from which larger and less frequent words can be composed. The vocabulary is computed using a sub-word 
tokenizer such as byte pair encoding that greedily merges commonly occurring sub-strings based on their frequency.

Each token in the vocabulary $\mathcal{V}$ is mapped to a word embedding, the same token always maps to the same 
embedding. 

To accomplish this, the $N$ input tokens are encoded in the matrix $T$ with size $|\mathcal{V}| \times N$, where the 
$n-$th column corresponds to the $n-$th token and is a $|\mathcal{V}| \times 1$ one-hot vector. 

The embeddings for the whole vocabulary are stored in a matrix $\Omega_e$ with size $D \times |\mathcal{V}|$. The input 
embeddings are computed as $X = \Omega_e \cdot T$, and $\Omega_e$ is learned like any other network parameter. 

A typical embedding size D is 1024, and a typical total vocabulary size $|\mathcal{V}|$ is 30000, so even before the main 
network, there are many parameters in $\Omega_e$ to learn.

Finally, the embedding matrix $X$ representing the text is passed through a series of transformer layers, called a 
transformer model. 


There are three types of transformer models:  
\begin{enumerate}
    \item \textbf{Encoders}: An encoder transforms text embeddings into representations that support a variety of tasks. 
        \textbf{BERT} (Bidirectional Encoder Representations from Transformers) is an example of an encoder model that 
        uses a vocabulary of 30,000 tokens. Input tokens are mapped to 1,024-dimensional word embeddings and processed 
        through 24 transformer layers, each containing a self-attention mechanism with 16 heads. Each head computes 
        queries, keys, and values of dimension 64. Encoder models like BERT utilize transfer learning: during 
        pretraining, the transformer’s parameters are learned through self-supervision on a large text corpus. The 
        objective of pretraining is for the model to acquire general statistical knowledge of language. The self-
        supervised task involves predicting missing words within sentences from a large dataset. This learned 
        representation of text can then be fine-tuned for specific NLP tasks.  
    \item \textbf{Decoders}: A decoder generates new tokens to extend an input sequence of text. For example, \textbf{GPT-
        3} constructs an autoregressive language model, where the joint probability of a sequence of tokens \((t_1, t_2,
        \dots, t_N)\) is factored into a product of conditional probabilities:  
        \begin{equation}
            P(t_1, t_2, \dots, t_N) = P(t_1) \cdot \prod_{n=1}^N P(t_n \mid t_1, \dots, t_{n-1})
        \end{equation}  
        This autoregressive model is generative, defining a probability distribution over text sequences and enabling the 
        generation of new, plausible text examples. To generate text, the model takes an input sequence and produces a 
        probability distribution over possible next tokens. The selected token is appended to the sequence and fed back 
        into the model, repeating the process iteratively to produce coherent and extended text.  
   \item \textbf{Encoder-Decoders}: Encoder-decoder models are designed for sequence-to-sequence tasks, where one text 
        sequence is transformed into another. A common application is **machine translation**, where text in one language 
        is translated into another.  
\end{enumerate}