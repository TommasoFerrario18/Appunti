\chapter{Feed Forward Neural Network}
\section{Introduction}
The \textbf{Feed Forward Neural Network} (FFNN) is a particular type of neural
network where each layer is connected to the next one without loop. We can think
at each layer as a vector to a vector function.

Each layer is composed by a set of neurons, where each neuron receives a set of input
from many others unit and computes its own activation rule.

We can think at a FFNN as a composition of many functions, where each function is
a layer. For example: if a FFNN try to approximate a function $f(x)$ and has $3$
hidden layer, we can represent it as:
\begin{equation*}
    f(x) = f^3(f^2(f^1(x)))
\end{equation*}
where $f^i$ is a function of $i$-layer. The overall length of hidden layers is
the \textbf{depth} of the model.

During the training process we want to learn a function $f(x)$ that, starting from
the training data, try to approximate the \textbf{target function} $f^*(x)$.

In the case of supervised problems, each instance $x$ is associated with a label
$y\sim f^*(x)$, therefore, the output layer at each point $x$ must produce a value
that is close to $y$.
\begin{note}
    It's important that is $y \sim f^*(x)$ and not $y=f^*(x)$ because we want a
    model that generalizes.
\end{note}

The behavior of the intermediate layers (\textbf{hidden layers}) are not directly
specified by the training data, but is the learning algorithm that must decide
how to use these layers to best implement an approximation of $f^*$. The dimensionality
of the hidden layers is called \textbf{width} of the model.

\subsection{Training process}
The training process can be summarized as follows: initially, the networks weights
are randomly initialized, then, for each training example $(x, y)$, the network
computes $f^*(x)$, then we compute the error $E(y, f^*(x))$ and we adjust all weights
by using the gradient approximate which is calculated with backpropagation.

\begin{note}
    During the training phase, the weights are adjusted every $n$ examples,
    where $n$ is the batch size. Weights are usually fixed by considering the
    elements of a batch to avoid overfitting.
\end{note}
\section{Weight Learning}
If $f(x)$ is \textbf{non-linear} function, than we can theoretically approximate
it with an hidden layer, the problem is to find the right weights.

The essence of supervised machine learning is the creation of functions that
can look at examples (instances) and produce generalizations (predict data never seen).
This is a searching task over all possible functions, this is complex problem to
solve. So, in general, we restrict the set of all possible functions to a specific
families, this is called \textbf{hypothesis classes}.

A strategy to solve non-linear problem using a linear model is to use a
transformation of the input space. To modify the data representation, we can
apply \textbf{non-linear transformation} ($\phi$) to them. In other words, given
an instance $x$, we can think of $\phi(x)$ as a new set of features describing $x$.

There are different way to define a transformation:
\begin{itemize}
    \item Use a predefine $\phi$ called \textbf{kernel}
    \item Define manually $\phi$ but it isn't convenient.
    \item We can learn it but it isn't a general transformation. This is the
          approach used in deep learning.
\end{itemize}

In deep learning the goal is to learn a transformation $\phi$, meanwhile SVM use
a Kernel that are already defined. So, we can describe this approach using the
following model:
\begin{equation}
    f(x, \theta, \omega) = \phi(x; \theta)^T \omega
\end{equation}
where $\theta$ are the parameters used to learn the transformation $\phi$ from a
broad class of functions, while $\omega$ are the parameters that map from $\phi(x)$
to the target.

\begin{note}
    In a deep learning model $\phi$ defines a hidden layer.
\end{note}

The prediction model is generally a high dimensional linear function $f(x): x \cdot W + b$,
here we have a listing:
\begin{itemize}
    \item \textbf{Sign function}: this function is used for binary classification:
          \begin{equation}
              \hat{y} = sign(f(x))
          \end{equation}
    \item \textbf{Sigmoid function}: this function is used for binary classification
          when we need a confidence on decision for log-linear binary classification.
          \begin{equation}
              \hat{y} = \frac{1}{1+e^{-f(x)}}
          \end{equation}
    \item \textbf{Softmax}: this is used for multi-class classification, suppose
          to have $k$ classes:
          \begin{equation}
              \hat{y} = softmax(f'(x)) = \frac{e^{f'(x)_i}}{\sum_{j=1}^k e^{f'(x)_j}}
          \end{equation}
\end{itemize}

\textbf{Pros} for linear models are that can be fit efficiently and reliably, because
gradient on linear model are easy to compute.

\textbf{Cons} models are restricted to linear functions so they works for data linear
separable, solved by a transformation.

\section{Kernel trick}
Transform the input space into an higher dimensional space can be useful to solve
non-linear separable problem. But, this approach increase the number of parameters
and the complexity of the model. To solve this problem we can map the input space
into a higher dimensional space using some particular functions called \textbf{kernels}.

\begin{note}
    There can be many transformations that allow the data to be linearly separated
    in higher dimensions, but not all of these functions are actually kernels.
\end{note}

This approach is functional because we can use the \textbf{kernel trick} to operate
in the original feature space without computing the coordinates of the data in a
higher dimensional space.

A general linear model would be rewritten as:
\begin{equation}
    \omega^T x + b = b+ \sum_{i=1}^m \alpha_ix^Tx^{(i)}
\end{equation}
where $b$ is the bayes, $\alpha_i$ is a coefficient, $x$ is a point, $x^(i)$ is
$i$-instance of training set and $m$ is the number of training instances.

To use this model on non-linear separable data, we need to apply a transformations $\phi$
to the input space, so the model becomes:
\begin{equation}
    f(x) = \omega^T \cdot x + b = b + \sum_{i = 1}^m \alpha_ix^Tx^{(i)} = b +
    \sum_{i=1}^m \alpha_i\phi (x^T) \cdot \phi(x^{(i)})
\end{equation}

The kernel trick override $\phi(x^T)\phi(x^{(i)})$ with $k(x^T, x^{(i)})$ without
computing $\phi(x)$ and it's more computational efficient compared to $\phi$.

Moreover kernel trick allows us to learn models that are nonlinear as a function
of $x$ using a convex optimization techniques that are guaranteed to converge
efficiently, this is possible because $\phi $ is fixed and we optimize only $\alpha_i$.
SVM uses only support vector and $\alpha_i$ regulates this, $alpha_i= 0$ if and
only if $i$ isn't a support vector.

Each algorithm that uses kernel is called \textbf{kernel machines} or \textbf{kernel
    methods}
\section{Gradient optimization}
Usually, in deep learning we want to minimize the \textbf{loss function} which in
term of operational research is the \textbf{objective function}. In order to
achieve this goal we use partial derivative $\frac{\delta}{\delta x_i} f(x)$ to
measure how the loss function changes as only the variable $x_i$ changes.

The \textbf{gradient} $\nabla f(x)$ generalizes the concept of derivative to the
case where the derivative is a vector. In other word, the gradient is a vector
where the element $i$ is the partial derivative of the function with respect to
the variable $x_i$.

Once we have defined that, we can use the direction opposite to the gradient to
minimize the loss function. This is called \textbf{gradient descent} and it's
defined as:
\begin{equation}
    x' = x - \eta \nabla f(x)
\end{equation}
where $x$ is the starting point, $x'$ is the next point, $\eta$ is the learning
rate and $\nabla f(x)$ is the gradient of the loss function.

\begin{note}
    The gradient descent converge when every element of the gradient is zero.
\end{note}

The learning rete is an hyperparameter that controls how much we are moving in the
direction of the gradient. The optimal value od $\eta$ change every time we update
the weights. But, do to computational cost, we cannot compute it every time, so we
use a fixed value. The best value of $\eta$ is the one that start from a high value
in order to explore different way and reduce the chance of getting stuck in a local
minimum, and then decrease it to converge to the global minimum.

Since in many cases the loss function is not convex, the loss function cannot be
optimized in a close form. So we need an iterative numerical optimization
procedure to find the minimum of the loss function. An example is \textbf{gradient
    descent}.

Sometimes the cost function may be a function that we cannot evaluate, for
computational reasons. In these cases, we can still use an iterative numerical
optimization procedure as long as we have some way of approximating the gradient.

Since the goal of our network is to approximate a function, we need to measure how
much the function we have learned differs from the target function. In order to
do this, we introduce a \textbf{loss function}.

Formally, this function $L(\hat{y}, y)$ return a scalar value that measures how
well the prediction $\hat{y}$ match the target $y$. For minimizing the optimization
problem over training sample, we add the parameters of the model $\theta$ to the
loss function:
\begin{equation}
    J(\theta) = \frac{1}{n} \sum_{i=1}^n L(\hat{y}_i; y_i; \theta)
\end{equation}
Since we have the label and the prediction, the main goal for the training algorithm
is to find the right set of parameters $\theta$ that minimize the loss function.
Having said this, we can rewrite the previous formula as follows:
\begin{equation}
    \hat{\theta} = \argmin_{\theta} J(\theta) = \argmin_{\theta} \frac{1}{n}
    \sum_{i=1}^n L(\hat{y}_i; y_i; \theta)
\end{equation}

In addition, for reducing the risk of overfitting, we can add a \textbf{regularization
    term} to the loss function:
\begin{equation}
    \hat{\theta} = \argmin_{\theta} \frac{1}{n} \sum_{i=1}^n L(\hat{y}_i; y_i; \theta)
    + \lambda R(\theta)
\end{equation}
where $\lambda$ is an hyperparameter that control the trade-off between bias and
variance, $R(\theta)$ is the regularization term which correspond to a scalar that
reflects the parameter complexity. The regularization term is used to prevent the
model from learning the noise in the training data. This term does not always
appear in the loss function, but we can apply it implicitly by changing the
network architecture or altering the data. Examples of these operations are:
\begin{itemize}
    \item Disable some connections in the network.
    \item Add noise to the input data.
\end{itemize}
\begin{note}
    A network that use small weights have better generalization properties than
    a network that use large weights, because small noise impact more on results
    when network has large weights.
\end{note}
\subsection{Stochastic Gradient Descent}
The \textbf{Stochastic Gradient Descent} (SGD) is a variant of the gradient descent
algorithm that is used to train neural networks. Since the computational cost
of computing the gradient of the loss function is $\mathcal{O}(n)$, where $n$ is
the number of training examples, the SGD is used to approximate the gradient of
the loss function by computing the gradient on a subset of the training data.

We can still update the gradient on every training example (Online Learning), but
this may result in a slow convergence and a noisy gradient.

To solve this problem, we can use a \textbf{mini-batch} which is a subset of the
training data. Typically, the mini-batch size is relatively small, ranging from
$1$ to few hundreds of examples. Using mini-batch, we can compute the gradient
of the loss function on the mini-batch and update the model parameters in a more
robust way.

We can compute the gradient of the loss function on the mini-batch as follows:
\begin{equation}
    g = \frac{1}{m} \sum_{i = 1}^{m} \nabla_{\theta} L(\hat{y}_i, y_i, \theta)
\end{equation}
and then we can update the model parameters as follows:
\begin{equation}
    \theta = \theta - \eta g
\end{equation}

When we choose the mini-batch size, we have to consider the trade-off between
the computational efficiency and the convergence speed. In general, the larger
the mini-batch size, the more accurate the estimate of the gradient, but the
slower the convergence. While the smaller the mini-batch size, the noisier the
estimate of the gradient, but the faster the convergence.
\section{Loss/Costo function}
\begin{definition}[Loss function]
    A \textbf{loss function} is a function that maps an event or values of one
    or more variables onto a real number intuitively representing the network
    performance on a single data point.
\end{definition}
\begin{definition}[Cost function]
    A \textbf{cost function} is a function that maps the average loss of the
    entire dataset onto a real number.

    A cost function must faithfully represent the purpose of the network.
\end{definition}

After we gave this definition, we can say that the goal of the training algorithm
is to minimize the cost function. In general, cost function should assume small
values to get better performance in the network.

There is a wide range of loss functions that can be used in deep learning, so we
need to choose the right one for our problem. The choice of the loss function
depends on the task we want to solve. Here are some examples of loss functions:
\begin{itemize}
    \item \textbf{Classification}:
          \begin{itemize}
              \item \textbf{Maximum likelihood}:
                    \begin{equation}
                        Loss = - \frac{1}{N} \sum_{i=1}^N y_i p(\hat{y}_i) +
                        (1-y_i) p(1-\hat{y}_i)
                    \end{equation}
              \item \textbf{Binary Cross-entropy}: also known as log loss, is used
                    for binary classification:
                    \begin{equation}
                        Loss = - \frac{1}{N} \sum_{i=1}^N y_i \log(p(\hat{y}_i))
                        + (1-y_i) \log(p(1-\hat{y}_i))
                    \end{equation}
              \item \textbf{Categorical Cross-entropy}: is used for multi-class
                    classification:
                    \begin{equation}
                        Loss = - \sum_{i=1}^N y_i \log(p(\hat{y}_{i}))
                    \end{equation}
          \end{itemize}
    \item \textbf{Regression}:
          \begin{itemize}
              \item \textbf{Mean Squared Error}: is used for regression problems
                    and is defined as:
                    \begin{equation}
                        Loss = \frac{1}{N} \sum_{i=1}^N (y_i - \hat{y}_i)^2
                    \end{equation}
              \item \textbf{Mean Absolute Error}: is used for regression problems
                    and is defined as:
                    \begin{equation}
                        Loss = \frac{1}{N} \sum_{i=1}^N |y_i - \hat{y}_i|
                    \end{equation}
              \item \textbf{Huber Loss}: is used for regression problems and is
                    defined as:
                    \begin{equation}
                        Loss = \frac{1}{N} \sum_{i=1}^N \begin{cases}
                            \frac{1}{2}(y_i - \hat{y}_i)^2                  & \text{if } |y_i - \hat{y}_i| \leq \delta \\
                            \delta |y_i - \hat{y}_i| - \frac{1}{2} \delta^2 & \text{otherwise}
                        \end{cases}
                    \end{equation}
          \end{itemize}
\end{itemize}
\begin{note}
    This is not an exhaustive list of loss functions, there are many other loss
    functions that can be used in deep learning. Also you can define your own loss
    function. In this case, is important that the loss function is differentiable.
\end{note}
\section{Output function}
The choice of the loss function is closely related to the choice of the output
unit. The output unit is the last layer of the network and it is responsible for
transform from the features to complete the task that the network must perform.

There are many output units that can be used in deep learning, here are some examples:
\begin{itemize}
    \item Linear
    \item Sigmoid
    \item Softmax
    \item Gaussian Mixtures
\end{itemize}
\subsection{Linear output unit}
One simple output unit is the linear unit, which is based on linear model:
\begin{equation}
    \hat{y} = W^T \cdot h + b
\end{equation}
This unit can be used to produce the mean of a Gaussian distribution, but it's
not suitable for classification problems because it can produce any real number.
\subsection{Sigmoid output unit}
The sigmoid output unit is used for binary classification problems. The output
of this unit is a value between $0$ and $1$ and it can be interpreted as the
probability of the input belonging to the positive class.

This function can be approximated using the linear output unit, but the function
we create would have 0 gradient outside the $[0, 1]$ range, so the learning
algorithm would not be able to update the weights.

The sigmoid function resolve the previous problem, because it ensure a strong
gradient whenever the model has a wrong answer. The sigmoid function is defined as:
\begin{equation}
    \sigma(x) = \frac{1}{1+e^{-x}}
\end{equation}
\subsection{Softmax output unit}
The softmax output unit is used for multi-class classification problems. The output
of this unit is a probability distribution over the classes. To implement this
we need to have a linear layer to predicts the un-normalized log probabilities and
then we apply the softmax function to normalize the output.

The softmax function is defined as:
\begin{equation}
    \sigma(x)_i = \frac{e^{x_i}}{\sum_{j=1}^k e^{x_j}}
\end{equation}

The softmax function is a good output function because is continuous and
differentiable.

Since the softmax function doesn't change the ordering of the output values, the
largest value before will still be the largest value after the normalization.

We can interprete the output of the softmax function as the probability of the
input belonging to the $i$-th class. Doing this, we need to consider a unknown
class, otherwise if we show to the model something that it has never seen, the
model will be confident that the input belongs to a class that it has never seen.
\subsection{Gaussian Mixtures output unit}
We often want to perform multimodal regressions, that is to predict real values
that came from a conditional distribution $p(y|x)$ that has more than one peack
in $y$ for the same value of $x$.

Then we can use a Gaussian Mixture Model which is defined as:
\begin{equation}
    p(y|x) = \sum_{i=1}^n p(c= i|x) N(y; \mu^{(i)}(x), \Sigma^{(i)}(x))
\end{equation}
where the NN output have the following value:
\begin{itemize}
    \item $p(c=i|x)$ is the mixture component.
    \item $\mu^{(i)}(x)$ is the mean of the $i$-th component.
    \item $\Sigma^{(i)}(x)$ is the covariance matrix of the $i$-th component.
\end{itemize}

\section{Activation Function}
The \textbf{hidden unit} applies an affine linear transformation such as:
\begin{equation}
    z = Wx + b
\end{equation}
where $W$ is the weight matrix, $x$ is the input vector and $b$ is the bias vector.
On the results of this transformation, we apply a non-linear function called
\textbf{activation function}.

\subsection{Tanh}
Hyperbolic Tangent is similar to sigmoidal function. The main difference between
them is that, the negative input are mapped in strongly negative value and zero
inputs are mapped in zero.

Some important characteristics are that the $\tanh$ is \textbf{differentiable}
and \textbf{monotonic}, but its derivative isn't \textbf{monotonic}.
\begin{note}
    The $\tanh$ function is used for \textbf{binary classification} problems.
\end{note}

Cause the form of the derivative values, which are greater than sigmoidal's derivative,
this function can help to speed up the training phase.
\subsection{Rectified Linear Unit}
Rectified Linear Unit (ReLU) is the most used activation function in neural networks.
This function is described as:
\begin{equation}
    g(x) = \max[0,z]
\end{equation}
We can see that ReLU is differentiable anywhere at the point where the value is
$0$. This is not a problem since we compute the gradient using approximation an
the probability to get $0$ is low.

ReLU allows us to simplify the model because we can get a sparse representation
of the network, due the possibility to deactivate some neurons. This is possible
thanks to negative input which will be converted to zero so it will be deactivated.

Since ReLU have a ``problem'' with the $0$ value, there are some generalization
that can be used to solve this. In order to do this, we can express the ReLU as
a generalization of the following formula:
\begin{equation}
    h_i = g_i(z,\alpha) = max(0,z_i) + \alpha_i min(0, z_i)
\end{equation}
Other versions of ReLUs are:
\begin{itemize}
    \item \textbf{Absolute value rectification}: this function is obtained by
          fixing $\alpha_i = -1$ to express $g(z) = |z|$. It is used to object
          recognition.
    \item \textbf{Leaky ReLU}: this function is obtained by fixing $\alpha_i = 0.01$.
          This one solves the problem of zero gradient which means that the model
          doesn't learn. This is done by Leaky ReLU with a small linear component
          for negative input.
    \item \textbf{Parametric ReLU}: this function is like Leaky ReLU but the
          parameter $\alpha_i$ is learned.
    \item \textbf{Maxout units}: the activation function $f$ is learned. It
          combines different ReLU to produce a convex function.
\end{itemize}
\subsection{Softplus}
Another generalization of the ReLU function is the Softplus function. This function
is defined as:
\begin{equation}
    f(x) = \log(1+e^x)
\end{equation}
This function was defined to solve ReLU's problem of non differentiable point,
it is a smooth approximation of ReLU. Theoretically we can think to get better
performance using this function compared to ReLU, but empirically it doesn't.

\subsection{Differences}
There are some guides to use these activation functions:
\begin{itemize}
    \item Sigmoids and their combinations generally works better in case of classifiers.
    \item Sigmoids and tanh Sometimes are avoided due the vanishing gradient problem.
    \item ReLU is the default activation function for hidden layers.
    \item We can use Leaky ReLU if we encounter a case of dead neurons.
    \item ReLU should only be used for hidden layer.
\end{itemize}
In general we can initially use ReLU, than if we cannot obtain better performance
we can change activation function.

\section{Initialization}
Another important step inside the planning of a deep neural network is the 
initialization process.

First of all we can initialize all \textbf{bias} to $0$ to discover eventually 
dead neurons. % ?

For \textbf{weights} we must not initialize to the same values because we have to
brake the symmetry of the model, so we can sample weights from a normal 
distribution with mean $0$ and variance dependant of the number of neurons in 
the previous layer.
\begin{equation*}
    W^{[l]} \sim \mathcal{N}\left(0, \frac{1}{n^{[l-1]}}\right)
\end{equation*}

\section{Regularization}
Exist a lot of regularization techniques that allowed us to increase model generalization.
Here we can see a list of all techniques:
\begin{itemize}
    \item Parameters norm penalties
    \item dataset augmentation
    \item noise robustness
    \item early stopping
    \item parameter tying and parameter sharing
    \item maltitask learning
    \item bagging and other ensemble methods
    \item dropout
    \item adversal training
\end{itemize}

We need to have a model that generilize because we have to get a low training error 
and a low test error, in this way we avoid overfitting and underfitting.

In the context of deep learning, most regularization stategies are based on 
\textbf{regularizing estimator}. This is done through reducing \textbf{variance} and
increasing \textbf{bias} of the estimator.

\begin{note}
    A good estimator is one that decreeses the variance without increasing too much the bias.
\end{note} 

When we use a model to solve a complex task, we can't have the generation distribution 
of datas used to train that model, so this means that the model could have high 
complexity and we could risk to enter in overfitting. 
It's difficult to find a better model with a low complexity while we are trying to 
solve complex task, so we can apply a regularizing process on a complex model to 
improve generalization.

The idea of regualrizing a model is to train a model not simply minimizing the loss
but also managing variability of datas. There are 2 way to do this:
\begin{itemize}
    \item \textbf{directly}: introducing prior knowledge or express generic preference 
    for a simpler model class. This is implemented by changing contrains, adding 
    restrictions on the parameter values or changing objective function by adding 
    soft contrains that penalize certain values of the parameters
    \item \textbf{indirectly}: data augmentation, introducing noise in the training
    data or using ensemble methods
\end{itemize}

\subsection{Parameters Norm Penalties}
We add a penalty to the objective function
$$\hat{J}(\theta) = J(\theta) + \alpha \Omega (\theta), \ \ \alpha\in [0,\infty)$$
$\alpha$ is a hyperparameter that regulates the contribution of norm penalties, 
also it may change for each layer.

During the training phase, we try to minimize the objective fucntion while 
we decrease some measure of the size of the parameters. It affects only a subset 
of parameters.

$\Omega$ usually penaliezes only the affine transformation at each layer ($W$) not the 
bais ($b$). We have $3$ different methods:
\begin{itemize}
    \item \textbf{sum of the weights} ($L_1$): pushes all values to $0$ penalizing 
    small weights more than big one.
    $$\Omega(w) = \sum_{w_j}|w_j|= \|w\|_1$$
    \item \textbf{sum of the sqared weights} ($L_2$): penalizes large values more
    $$\Omega(w) = \sqrt{\sum_{w_j}|w_j|^2} = \|w\|_2$$
    \item \textbf{$p$-norm} ($L_p$): for $p<2$ encourage sparser vector while larger 
    values of $p$ discourage large weights more
    $$\Omega(w) = \sqrt[p]{\sum_{w_j}|w_j|^p} = \|w\|_p$$
\end{itemize}

$L_2$ shrinks weights on features that add noise to the model (feature with high variance 
but low covariance with target), while $L_1$ provides a solution that is sparse,
this property can be seen as a features selection mechanism.

\begin{note}
    using $L_2$ negative and positive weights moves towards 0 while using $L_1$ 
    negative and positive weights moves towards 0 regardless of magnitude.  
\end{note}

We can add to the previous model a contraint on regularization term, for example 
$$\Omega(\theta) \le k$$
where $k$ is a small value. In this way the optimization problem changed because 
we have to consider also the upperbound of regularization term. To solve this problem
we can use \textbf{lagrangen function} and insert penalties into objective 
function.
$$\mathcal{L}(\theta, \alpha; X,y) = J(w;X,y)+ \alpha(\Omega(\theta)-k) $$
where $\alpha$ is the lagrangean multiplier and formally the optimization problem 
will be 
$$\argmin_\theta \max_{\alpha \ge 0} \mathcal{L}(\theta, \alpha)$$ 
To solve this problem we can use techniques that minimize $J$ and then project 
the solution obtained to the feasible reagion $\Omega(\theta) -k$, this method prevent 
to get stuck in local minima and impose stability on the optimization procedure.

\subsection{Dataset augmentation}
We can get better generalization training model on more data. The main problem is 
that data is limited also labelling is an extremely tedious task. Dataset 
augmentation provides a cheap easy way to increase the amount of your training data.
Moreover, when you are comparing different algorithms you have to compare on
augmented and non augmented dataset to avoid bias.

Exists a lot of algorithms for data augmentation use case dependant.

\subsection{Noise robustness}
Noise injection can be thought of as a form of regularization. When we add a noise 
with infinitesimal variance at input of the model is equivalent to imposing a penalty
on the norm of the weights. 

Noise can be injected at different levels of deep models, if we add on input we are 
defining a data augmentation procedure, so we have to do the same best practise 
explained before.

Adding small noise pushes model into regions where the weights model are insensitive to small 
variation of input finding points of local minima surrounded by flat regions.

We can add noise on labels by getting in output a level of confidence (probability),
an example of this method implemented analitically in the cost function is \textbf{cross entropy} 
Another example of this is \textbf{label smoothing} where we use softmax AF in
the output layer to get a probability in place of get descrete output.

$$\left[\begin{array}{c}
    1\\0\\\vdots\\ 0
\end{array}\right]_{lable} \implies \left[\begin{array}{c}
    0.9\\0.01\\\vdots\\ 0.02
\end{array}\right]_{lable}$$

Using a softmax we will never predict 0 or 1 but it will predict a probability so 
model can always learn and make more extreme prediction forever. This has the advantage 
of preventing the pursuit of hard probabilities without discouraging correct classification.

\subsection{Multitasking learning}
Multitasking learning is a way to improve generalization by pooling the 
examples arising out of several tasks. Multitasking learning is usually performed 
through the following architecture:
\begin{itemize}
    \item task-specific parameters: which only benefit from the examples of their
    task to achieve good generalization
    \item generic parameters: parameters shared across all tasks which benefit from 
    the pooled data of all tasts.
\end{itemize}
Multitasking learning is a form of parameter sharing. Shared parameters improve 
generalization in proportion of number of examples for the general task. Additional 
task imposes contraints on the parameters in the shared layers preventing overfitting.
Note that improvement in generalization only occurs when there is something shared across
the tasks at hand.

\subsection{Early stopping}
When training large models with sufficient representational capacity to overfit 
the task, we often observe that training error decreases over time but validation 
error begins to rise again. We can obtain a model with better validation error by
returning to the parameter setting at the point in time with the lowest validation 
error.

This is one of the most techniques used. Early stopping can be used as follow:
\begin{itemize}
    \item choose number of iterations $n$
    \item train model for $n$ iterations 
    \item validate model and compute validation error, compare consecutive validation
    error and return to point 2 until delta is insignificant or negative.
\end{itemize}
Every iteration model improves its weights until it starts to going in overfitting.
\begin{note}
    It's impossible to write all parameters on disk during training.
\end{note}

\subsection{Parameters Tying}
Parameters Tying refers to explicitly forcing the parameters of 2 models to be close to each other
using the norm penalty;
$$\|w(A)- w(b)\|$$
with $A$ and $B$ respectively first and second model. This norm penalty is added 
to the objective function like a constraint. It is used in Siamese networks, convolution 
operators and multitasking learning.sÃ¬

\subsection{Bagging}
It's a technique for reducing generalization error combining several models (\textbf{ensemble model}). 
You have to train $k$ different models on $k$ different subsets of training data
with same number of examples as original dataset using a random sampling with replacement.
Every model is then tested, they produce $k$ votes for the final result. These 
models are called ensemble.

The objective of this method is to introduce generalization by using different models 
trained on different examples using different initialization hyperparameters and different 
structures to have indipendant errors.

Cons of this method is that ensemble models doesn't provide us a scalable way to 
improve performace, moreover we have 2-3 models inside the ensemble.

\subsection{Dropout}
Dropout provides a computationally inexpensive but powerful method of regularizing
a broad family of models. It provides an inexpensive approximation of a ensemble with 
a high number of subnetworks of the starting one. All subnetworks should be formed by removing 
non-output units from underlying base network, generating an exponential number of 
models using a trattable amount of memory. Dropout removes the need to 
accumulate model votes at the inference stage and it is a sort of method to force the 
model to learn with missing input and hiddden unit.

To do a train phase we use a \textbf{minibatch-based learning algorithm} that makes 
small steps, such as stochastic gradient discent.

Each time we change minibatch we \textbf{randomly indipendent sample} a different binary mask to apply
to all imput and hidden units in the network. Typically the distribution of including 
a unit is $0.5$ for hidden unit and $0.8$ for input unit.

We can have some problem, infact at training time we are required to divide the 
output of each unit by the probability of that unit's dropout mask, so we assign 
as likelihood on the output. In this way we make sure that the expeted total 
input to a unit at test time is the same as the expeted total input of that unit 
at train time.

We haven't theoretically basis for the accuracy but empirically it performes well.
Complexity of dropout is limited to a $\mathcal{O}(n)$ computation per example 
to update because it needs to generate a total of $n$ random numbers and multiply them
by the state. 

Dropout can be used with every model and also with stochastic gradient descent.
The cost of applying dropout to the entire model could be significant, but the amount of 
benefit on generailization error counldn't be enought to the cost of the regularization.

\subsection{Adversal training}
We can add adversal examples that are perturbed examples of train set. These
are used to train the network and will allowed us to reduce test errors. Adversal 
examples can be generated by adding to a bit of perturbation on some train example 
with a low moltiplication factor of noise. This method allowed us to 
generate new examples for training from the original train set with slightly changes
that are unrecognisable by humans.

\subsection{Generative Adversal Models}
GAN are generative model that implicitly learn the \textbf{data distribution}
from a zero-sum game between 2 compiting neural network:
\begin{itemize}
    \item \textbf{discriminator}: a neural network that classify examples between 
    fake and real. 
    \item \textbf{generator}: generate fake example that are given as input to the 
    discriminator
\end{itemize}

The target of the generator is to generate example the most similar to real one 
in such a way to be classified in real example by the discriminator. Quite the
opposite the discriminator have to identify real examples from fake one. If the 
discriminator makes a mistake so we will update discriminator's weights, vice versa we will
update generator's weights
 