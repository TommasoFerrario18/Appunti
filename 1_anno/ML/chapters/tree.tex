\chapter{Alberi decisionali}
Vediamo come sfruttare una struttura dati discreta, l'albero di decisione, per
affrontare problemi di concept learning. Per la costruzione di queste strutture
utilizzeremo l'algoritmo chiamato \textbf{ID3}, il quale costruisce l'albero
scegliendo gli attributi in base al valore dell'\textbf{information gain}. In
questo caso, a differenza del concept learning in cui si analizza un'istanza alla
volta, si prendono in considerazione gruppi di dati.

Gli attributi possono assumere diversi valori. Un albero decisionale è formato da:
\begin{itemize}
    \item \textbf{Nodes} (nodi): rappresentano gli attributi da testare.
    \item \textbf{Branches} (rami): sono etichettati con i possibili valori
          dell'attributo specificato nel nodo sorgente del ramo.
    \item \textbf{Leaf nodes} (foglie): contengono i valori della classificazione.
\end{itemize}

Possiamo quindi scegliere, al posto delle classiche funzioni booleane, alberi di
decisione per rappresentare un modello che applicato ad esempi non visti ci dirà
se applicare in output un'etichetta vera o falsa in base a quanto appreso. Per
classificare un'occorrenza si parte dal nodo radice dell'albero, si verificano
i tratti indicati da questo nodo e si procede lungo il ramo dell'albero
confrontandoli con il valore dell'attributo. Si ripete questo controllo fino ad
arrivare alle foglie.

La flessibilità nella costruzione dell'albero sta nel scegliere gli attributi e
i valori di ognuno. Con l'algoritmo ID3 si costruiscono alberi decisionali in base
alle istanze che ricevo. Formule booleane possono essere rappresentate in un albero
decisionale, costruendo un albero che fornisca la risposta yes solo nei casi in
cui la formula booleana sia vera.

Riassumiamo alcune caratteristiche degli alberi decisionali:
\begin{itemize}
    \item Abbiamo attributi con valori discreti.
    \item Abbiamo un target di uscita discreto, le foglie hanno valori precisi.
    \item Posso costruire ipotesi anche con disgiunzioni.
    \item Può esserci “rumore” nel training dei dati.
    \item Possono esserci attributi di cui non ho informazioni.
\end{itemize}

Un percorso dalla radice fino alla foglia mi rappresenta una congiunzione di
attributi, mentre analizzando l'albero nella sua interezza posso definirlo come
una disgiunzione di congiunzioni. Un albero quindi formalizza la congiunzione di
vincoli su attributi, percorso da radice a foglia, ma può anche estendere il
linguaggio a disgiunzioni di vincoli su attributi.

Posso avere alberi diversi a seconda della scelta del nodo radice.
\begin{teorema}
    Con un albero decisionale posso rappresentare tutte le funzioni booleane.
\end{teorema}

Possiamo dire che gli alberi decisionali descrivono tutte le funzioni booleane.
Avendo $n$ attributi booleani avremo un numero distinto di tabelle di verità,
ciascuna con $2^n$ righe, pari a:
\begin{equation}
    \text{Numero di alberi} \ = \ 2^{2^n}
\end{equation}
Usando funzioni booleane posso convertire tabelle di verità in alberi decisionali,
con quindi ogni percorso dalla radice ad una foglia che rappresenta una regola e
l'intero albero che rappresenta la congiunzione di tutte le regole. Le foglie
quindi sono gli assegnamenti di verità della tabella.

Vediamo quindi un algoritmo generale per la costruzione dell'albero:
\begin{enumerate}
    \item Si inizia con un albero vuoto.
    \item Scelgo un attributo opportuno per fare lo split dei dati.
    \item Per ogni split dell'albero:
          \begin{itemize}
              \item Se non c'è altro da fare si fa la predizione con l'ultimo nodo
                    foglia.
              \item Altrimenti si torna allo step 2 e si procede con un altro split.
          \end{itemize}
\end{enumerate}

Una strategia per selezionare l'attributo per fare lo split, può essere quella
“a maggioranza”, ovvero prendendo l'attributo che con i suoi valori ha meno
disomogeneità. Per farlo calcolo la differenza tra yes e no di ogni valore per
un certo attributo, sommandone i risultati. Scelgo l'attributo con più omogeneità,
che ha una distribuzione più pulita dei risultati.

Idealmente, un buon attributo divide gli esempi in due sotto-insiemi che sono
composti tutti da istanze positive o da istanze negative.

Un altra metodologia per scegliere come effettuare lo split consiste nell'utilizzo
del \textbf{Gini Index}.
\begin{definizione}
    Per un insieme di istanze posso definire il \textbf{Gini index} come:
    \begin{equation}
        Gini(D) = 1 - \sum_{i = 1}^c p_i^2
    \end{equation}
    dove: \begin{itemize}
        \item $D$ è l'insieme di istanze.
        \item $c$ rappresenta il numero di classi.
        \item $p_i$ è la proporzione di istanze nella classe $i$ che sono presenti
              nel dataset.
    \end{itemize}

    Il valore di questo indice è compreso tra 0 e 1, dove 0 indica che nell'insieme
    sono presenti solo elementi di una classe, mentre 1 indica che nelle classi
    sono presenti istanze diverse.
\end{definizione}
Questo indice può essere utilizzato per calcolare la \textit{purezza} dell'intero
dataset, oppure per calcolare la \textit{purezza} delle partizioni che si creano
utilizzando un determinato attributo per fare lo split.

Per selezionare che attributo utilizzare per lo split tramite l'indice di Gini
si calcola una somma pesata per ogni attributo:
\begin{equation}
    WeightedSum = \sum_{i} p_i \cdot Gini_i
\end{equation}
dove:
\begin{itemize}
    \item $p_i$: rappresenta la probabilità di osservare uno specifico valore.
    \item $Gini_i$: rappresenta l'indice di Gini calcolato dove l'attributo assume
          il valore osservato.
\end{itemize}
\section{Algoritmo ID3}
Lo scopo di questo algoritmo è quello di trovare un albero, di dimensioni ridotte,
che sia consistente con gli esempi di training. L'idea alla base di questo algoritmo
è quella di scegliere, in modo ricorsivo, l'attributo più significativo come radice
del sotto-albero.

Si fanno quindi crescere in modo coerente gli alberi piccoli scelti in partenza.
Si punta ad arrivare ad un albero valido per tutti gli esempi ricevuti e anche
per quelli non visti.

Iniziamo a vedere l'algoritmo anche se saranno necessarie molte specifiche:
\begin{algorithm}[!ht]
    \begin{algorithmic}
        \Function{ID3}{}
        \State $A \gets$ \textit{il ``miglior'' attributo di decisione per il
            prossimo nodo}
        \State \textit{Assegno A come attributo di decisione per il nodo}
        \For {\textit{Ogni valore dell'attributo A}}
        \State \textit{Creo un discendente}
        \EndFor
        \State \textit{Ordina gli esempi di training alla foglia in base al valore
            dell'attributo del branch}
        \If {\textit{Ho classificato tutti gli esempi di training}}
        \State \textit{Mi fermo}
        \Else
        \State \textit{Itero sulle foglie appena create}
        \EndIf
        \EndFunction
    \end{algorithmic}
    \caption{Algoritmo ID3}
\end{algorithm}

Per la selezione del miglior attributo, bisogna capire cosa si intende come
attributo migliore. Per farlo introduciamo la seguente notazione:
\begin{equation}
    [\text{esempi positivi}+, \text{esempi negativi}-]
\end{equation}
entrambi rappresentati con un valore intero.

Per essere ancora più precisi bisogna considerare l'\textbf{entropia} degli insiemi,
ovvero il contenuto informativo dell'insieme.
\begin{definizione}
    Dato un training set $S$ contenete i valori $v_i, \ i = 1 \dots n$. Possiamo
    definire l'\textbf{entropia} (Contenuto informativo) di un insieme $S$ come:
    \begin{equation}
        H[V] = I(P(v_1), \dots, P(v_n)) = \sum_{i = 1}^n - P(v_i) \log_{2} P(v_i)
    \end{equation}

    Dove $P(y)$ rappresenta la probabilità che il valore $y$ sia presente.

    Nel caso booleano le istanze presenti in un certo insieme $S$ sono associate
    ad un'etichetta, conteggiandole. Si utilizza una variabile $p$ per rappresentare
    i valori di $S$ a cui è associata un'etichetta positiva, mentre, si utilizza
    la variabile $n$ per rappresentare i valori di $S$ a cui è associata un'etichetta
    negativa. Posso quindi riscrivere la sommatoria nel caso booleano come:
    \begin{equation}
        I\left(\frac{p}{p + n}, \frac{n}{p + n}\right) = -\frac{p}{p + n} \log_2
        \left( \frac{p}{p + n} \right) - \frac{n}{p + n} \log_2 \left(\frac{n}{p
            + n}\right)
    \end{equation}
    Se inoltre diciamo che $p_{+}$ è la proporzione di esempi positivi e $p_{-}$
    di quelli negativi possiamo misurare l'impurità di $S$ utilizzando l'entropia:
    \begin{equation}
        Entropy(S) = - p_{+} \log_2 p_{+} - p_{-} \log_2 p_{-}
    \end{equation}
    Avrò quindi alta entropia se positivi e negativi sono bilanciati.
\end{definizione}

A volte risulta utile calcolare l'entropia condizionata al valore di un attributo.
Questo si può fare utilizzando la seguente formula:
\begin{equation}
    H[Y | X] = \sum_{i = 1}^n p(X = x_i) \cdot H[Y | X = x_i]
\end{equation}

Con il termine \textbf{information gain} IG ci riferiamo al valore che viene
calcolato su ogni attributo $A$ presente nelle istanze contenute in $S$:
\begin{equation}
    IG(S, A) = I\left(\frac{p}{p + n}, \frac{n}{p + n}\right) - remainder(A)
\end{equation}
dove $remainder(A)$ rappresenta la somma pesata sul totale delle entropie calcolate
sui sotto-insiemi che si vanno a creare se si sceglie $A$ come attributo per lo
split dei dati.
\begin{equation}
    remainder(A) = \sum_{i=1}^v \frac{p_i + n_i}{p + n} \cdot I\left(\frac{p_i}
    {p_i + n_i}, \frac{n_i}{p_i + n_i}\right)
\end{equation}
Si sceglie l'attributo il valore dell'information gain più alto.

Facciamo qualche osservazione finale sull'algoritmo ID3:
\begin{itemize}
    \item Lo spazio delle ipotesi è completo e sicuramente contiene il target.
    \item Ho in output una singola ipotesi.
    \item Non si ha backtracking sugli attributi selezionati, si procede con una
          ricerca greedy.
    \item Fa scelte basate su una ricerca statistica, facendo sparire incertezze
          sui dati.
    \item Il bias non è sulla classe iniziale, essendo lo spazio delle ipotesi
          completo, ma sulla scelta di solo alcune funzioni, preferendo alberi
          corti e posizionando attributi ad alto information gain vicino alla
          radice. Il bias è quindi sulla preferenza di alcune ipotesi. Si usa il
          criterio euristico di rasoio di Occam.
    \item H è l'insieme potenza delle istanze X.
\end{itemize}
Viene introdotto però l'\textbf{overfitting}.
\begin{definizione}
    Definiamo formalmente l'\textbf{overfitting} come l'adattamento eccessivo,
    ovvero quando un un modello statistico molto complesso si adatta al campione
    perché ha un numero eccessivo di parametri rispetto al numero di osservazioni.
    Si ha quindi che un modello assurdo e sbagliato può adattarsi perfettamente se
    è abbastanza complesso rispetto alla quantità di dati disponibili.

    Nel machine learning se il learner viene addestrato troppo a lungo il modello
    potrebbe adattarsi a caratteristiche che sono specifiche solo del training set,
    ma che non hanno riscontro nel resto dei casi quindi le prestazioni sui dati
    non visionati saranno drasticamente peggiori. L'opposto è l'underfitting.
\end{definizione}
Se misuro l'errore di una ipotesi $h$ sul training set ($error_{train}(h)$) e poi
misuro l'errore di quella ipotesi sull'intero set delle possibili istanze $D$
($error_D(h)$) ho che l'ipotesi $h$ va in overfit sul quel data set se:
\begin{equation}
    error_{train}(h) < error_{train}(h') \ \land \ error_D(h) > error_D(h')
\end{equation}
quindi vado in overfitting se: presa un'altra ipotesi $h'$ questa presenta un
errore maggiore di $h$ sul training set, ma l'errore commesso sull'intero dataset
è minore di $h$. Il problema è che non posso sapere se esiste tale $h'$.

Per evitare il problema uso sempre il rasoio di Occam scegliendo ipotesi semplici
ed evitando di far crescere l'albero quando lo “split” non è statisticamente
significativo.

Un altro modo è quello di togliere pezzi, all'albero, che toccano poche istanze
o pure calcolare una misura di complessità dell'albero, minimizzando la grandezza
dell'albero e gli errori del training set, usando il \textbf{Minimum Description
    Length} (MDL).

In ID3 quindi posso scegliere sia in base all'information gain massimo o
all'entropia minima tra gli attributi.