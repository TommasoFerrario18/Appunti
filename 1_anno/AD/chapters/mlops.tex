\chapter{MLops}
Una tipica pipeline per un progetto di machine learning è composta da diverse fasi:
\begin{itemize}
    \item Comprensione del problema
    \item Raccolta dei dati
    \item Comprensione dei dati
    \item Preparazione dei dati
    \item Modellazione
    \item Valutazione
    \item Deployment
\end{itemize}
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.5\textwidth]{./img/MLops/CRISP-DM.png}
    \caption{ML Pipeline}
    \label{fig:ml_pipeline}
\end{figure}
In questa parte del corso ci occuperemo della parte di gestione dei dati, nello
specifico analizzeremo la parte di \textbf{Data Understanding} e \textbf{Data
    Preparation}.

Avere i dati corretti è una parte fondamentale per la creazione di un modello di
che sia in grado di ottenere dei risultati corretti.
\begin{center}
    \textit{Garbage in, garbage out}
\end{center}
I dati di training e testing devono derivare dai dati reali che verranno raccolti
in fase di production.
\section{Pipeline}
Una volta ottenuti i dati reali dovremo separarli in:
\begin{itemize}
    \item \textbf{Training data}: vengono utilizzati per addestrare e valutare
          il modello, sono quindi di dimensioni elevate e vengono raccolti
          attraverso diverse fonti. Spesso caratterizzati da un elevato throughput.
    \item \textbf{Serving data}: raccolti e utilizzati per fare previsioni in
          fase di production. Questa tipologia di dati è spesso caratterizzata
          da una bassa latenza.
\end{itemize}
Entrambi questi tipi di dati devono passare attraverso una fase di preparazione.
In questa fase si vogliono stabilire quali sono le informazioni principali,
quali dati sono rilevanti e quali dati sono irrilevanti. Si effettua dunque una
prima analisi. Inoltre, vengono eseguite diverse operazioni per far si che i dati
si adattino al modello che si vuole creare.

Prima di arrivare alla parte di addestramento del modello, si passa attraverso una
fase di validazione dei dati. Questa fase è necessaria in quanto i dati cambiano
o cambia il contesto.

L'obiettivo principale è quello di capire quali feature sono significative
per il modello e quali no. Questo serve per controllare in futuro se queste
feature significative sono cambiate.

Questa fase viene fatta confrontando le distribuzioni delle features ritenute
significative durante il training con la distribuzione delle stesse feature prese
dai dati raccolti in produzione.

In questo modo possiamo segnalare quando le distribuzioni cambiano e nel caso devo
adottare delle soluzioni:
\begin{itemize}
    \item Riaddestrare il modello.
    \item Avvisare l'utente che qualcosa sta cambiando.
    \item Non fare niente.
\end{itemize}
Può capitare nel tempo i le feature cambino il tipo (ad esempio si passa da int
a float), quindi, nella fase di validation, bisogna correggerli per renderli
compatibili col modello.

Per effettuare tutti i passi della pipeline servono diverse figure lavorative:
\begin{itemize}
    \item \textbf{Machine Learning Expert}
    \item \textbf{Software Engineer}
    \item \textbf{Site Reliability Engineer}: esperto che si occupa di gestire
          tutta la pipeline e di risolvere eventuali problemi.
\end{itemize}
\section{Data Acquisition}
Una delle fasi più importanti è \textbf{Data Acquisition}. In questa fase possiamo
avere diversi problemi come ad esempio l'introduzione di bias nei dati che
invalidano i risultati del modello.

In aggiunta, è obbligatorio sapere la \textbf{Data Provenance} ovvero documentare
come ho costruito il dataset di training e validation. Questo consiste nel sapere
quali sono i dati utilizzati, da dove vengono, come ho costruito il dataset. In
questo modo si risolvono eventuali problemi dovuti a risultati sballati per dati
che sono stati iniettati nel dataset.
\section{Data Understanding}
Una volta acquisiti i dati, il primo passo consiste nella loro interpretazione,
quindi di passa per una fase di \textbf{data exploration}.

Queste operazioni vengono solitamente svolte sui dati di training e di Serving
in modo da capire se i dati sono consistenti e se sono stati raccolti in modo
corretto, questo prende il nome di \textbf{sanity checks}.

I dati che vengono raccolti sono solitamente rappresentati sotto forma tabellare,
e possono essere:
\begin{itemize}
    \item \textbf{Discreti}: valori che possono essere rappresentati da un insieme
          finito o numerabile di valori. Possiamo inoltre avere anche due sotto
          categorie:
          \begin{itemize}
              \item \textbf{Categorici}: valori che rappresentano delle categorie.
              \item \textbf{Count}: valori che rappresentano il numero di
                    occorrenze di un certo evento.
          \end{itemize}
    \item \textbf{Continui}: valori che possono essere rappresentati da un insieme
          non numerabile di valori.
\end{itemize}
L'esplorazione dei dati avviene attraverso dei \textbf{sanity checks}, ovvero dei
controlli che vengono svolti per studiare l'integrità dei dati. Un esempio di sanity
check può essere il controllo della latitudine e longitudine di un punto, questi
valori devono essere compresi in un certo range.

Un ulteriore controllo viene fatto sul bilanciamento delle features, in particolare,
per quelle categoriche si controlla se la distribuzione degli esempi per ogni
categoria è bilanciata. In caso contrario, la feature potrebbe non essere significativa
oppure si potrebbe avere un bias nei dati.

Un altro controllo che viene fatto è il controllo della cardinalità delle features.
Ad esempio, una persona non può avere associata più di una data di nascita.

Tutti questi controlli possono essere essere effettuati attraverso diverse modalità:
\begin{itemize}
    \item \textbf{Visualizzazione}: attraverso grafici e tabelle.
    \item \textbf{Query SQL}: attraverso query che permettono di estrarre
          informazioni dai dati.
    \item \textbf{Script}: attraverso script che permettono di estrarre
          informazioni dai dati.
\end{itemize}

Per gestire le inconsistenze scoperte dai sanity checks si possono adottare diverse
strategie:
\begin{itemize}
    \item Eliminare l'esempio, questa strategia può essere adottata solamente
          nel caso in cui il numero di esempi eliminati non sia troppo elevato.
    \item Correggere l'errore con metodi di imputazione, bisogna però stare
          attenti perché non sempre si può fare. Per esempio, se i dati
          rappresentati da una feature sono vincolati da qualche normativa,
          non posso correggere l'errore con un metodo di imputazione.
\end{itemize}
Riassumendo, la fase di data exploration è composta dai seguenti passi:
\begin{itemize}
    \item \textbf{Identificazione delle variabili}: quali sono le variabili che ho
          a disposizione e che ruolo hanno.
    \item \textbf{Analisi univariata}: analisi delle variabili una alla volta.
          Voglio capire se la variabile in analisi è continua o discreta,
          studiare la sua distribuzione e capire se ci sono degli outlier. Questo
          può essere fatto attraverso l'utilizzo di box plot e istogrammi.
    \item \textbf{Analisi bivariata}: analisi tra coppie di variabili. Voglio
          capire come le variabili interagiscono tra di loro. Questo può essere
          fatto attraverso l'utilizzo di scatter plot.
    \item Capire se ci sono valori mancanti e come gestirli.
    \item identificare e gestire i valori anomali (outlier).
\end{itemize}
\begin{definizione}
    L'\textbf{Exploratory Data Analysis} (EDA) è una tecnica statistica che permette
    di esplorare i dati e di estrarre informazioni utili per la costruzione di un
    modello.
\end{definizione}

Tra le varie tecniche di analisi dei dati abbiamo:
\begin{itemize}
    \item \textbf{Measure of dispersion}: permette di capire quanto i dati sono
          distanti tra di loro. Questo può essere fatto attraverso l'utilizzo
          di varianza e deviazione standard e altre misure statistiche.
    \item \textbf{Box plot}: permette di visualizzare la distribuzione dei dati
          attraverso quartili.
    \item \textbf{QQ plot}: permette di confrontare la distribuzione dei dati
          con una distribuzione normale.
    \item \textbf{Scatter plot}: permette di visualizzare la relazione tra due
          variabili.
    \item \textbf{Swarm plot}: permette di visualizzare la distribuzione dei dati
          in base a una variabile categorica.
\end{itemize}
Le visualizzazioni sono spesso guidate dai dati, quando si fanno delle analisi
la comprensione di cosa si sta analizzando è fondamentale.

Queste analisi sono fondamentali per capire se i dati sono soggetti a dei bias.

È importante effettuare tutte queste analisi durante tutto il data lifecycle,
questo perché mi aiuta ad individuare eventuali problemi e a correggerli.

Le metodologie descritte in questo capitolo non rappresentano un processo che deve
essere seguito alla lettera, ma sono delle linee guida che possono essere adattate
in base al contesto. Ogni dominio applicativo richiede delle analisi specifiche.
In aggiunta, quando addestriamo un modello di machine learning dobbiamo capire se
il suo comportamento è \textbf{fair}, ovvero se il modello non è influenzato da
bias.
\section{Data validation}
Nella fase di sviluppo e mantenimento di un modello dobbiamo sempre controllare
che la distribuzione dei dati non sia cambiata durante il tempo.
Ovvero, che i valori su cui vogliamo effettuare delle previsioni abbiano la stessa
struttura dei dati di training.

Questa analisi è molto importante perché se si modifica un dato allora dobbiamo
accorgercene e modificare il modello di conseguenza.

Un esempio di cambiamento può essere il seguente:
\begin{itemize}
    \item Abbiamo una feature di tipo string che passa da Upper case a lower case.
    \item Le variabili che cambiano di semantica. Ad esempio il numero di giorni
          che diventa numero di ore.
    \item Scompare un attributo, diventa null.
\end{itemize}
Risulta quindi fondamentale avere un sistema di riconoscimento delle anomalie
nei dati. Questo sistema deve essere in grado di riconoscere quando i dati cambiano
e deve essere in grado di produrre un \textbf{alert}. 

Solitamente, in relazione con gli alert si costruisce un \textbf{playbook} che
specifica chi deve essere avvisato e come risolvere il problema.

In questo modo, quando si riceve un alert si sa già come risolverlo, seguendo 
le specifiche descritte nel playbook.