\chapter{Ripasso Algebra Lineare}
\begin{definizione} [\textbf{Spazio vettoriale}]
    Un insieme $V$ si dice \textbf{spazio vettoriale} se sono definite su $V$ due
    operazioni che godono di particolari proprietà:
    \begin{itemize}
        \item \textbf{Somma}: $+: V \times V \rightarrow V$
        \item \textbf{Prodotto per uno scalare}: $\cdot : V \times \mathbb{R} \rightarrow V$
    \end{itemize}
\end{definizione}

Le operazioni dello spazio vettoriale godono di alcune proprietà:
\begin{itemize}
    \item \textbf{Somma}:
          \begin{itemize}
              \item \textbf{Commutativa}: $\forall u,v \in V, u+v = v+u$
                    \begin{proof}
                        $\forall u,v \in V:$
                        $$ u+v = \left[\begin{array}{c}
                                    u_1 \\u_2\\ \vdots \\u_n
                                \end{array}\right] + \left[\begin{array}{c}
                                    v_1 \\v_2\\\vdots\\v_n
                                \end{array}\right] = \left[\begin{array}{c}
                                    u_1 + v_1 \\u_2 + v_2\\\vdots\\u_n+v_n
                                \end{array}\right] = \left[\begin{array}{c}
                                    v_1 + u_1 \\v_2 + u_2\\\vdots\\v_n+u_n
                                \end{array}\right] = \left[\begin{array}{c}
                                    v_1 \\v_2\\\vdots\\v_n
                                \end{array}\right] + \left[\begin{array}{c}
                                    u_1 \\u_2 \\\vdots\\u_n
                                \end{array}\right] = v+ u$$
                    \end{proof}
              \item \textbf{Associativa}: $\forall u,v,z \in V, (u+v)+z = v+(u+z)$
                    La dimostrazione si può fare facilmente utilizzando sempre la proprietà
                    associativa della somma tra scalari.
              \item \textbf{Esistenza dell'elemento neutro}: $\exists 0 \in V: 0+v = v,
                        \forall v\in V$
              \item $\forall u \in V, \exists w\in V \text{ unico}: u+w=0$
          \end{itemize}
    \item \textbf{Prodotto per uno scalare}:
          \begin{itemize}
              \item \textbf{Distributivo rispetto alla somma}: $\forall u,v \in
                        V, \forall \lambda \in \mathbb{R}, \lambda(u+v) = \lambda
                        u+\lambda v$
              \item \textbf{Distributivo rispetto alla somma tra scalari}:
                    $\forall u \in V, \forall \lambda,\mu\in \mathbb{R}: (\lambda +
                        \mu)u = \lambda \cdot u + \mu \cdot u$
              \item \textbf{Associativo rispetto al prodotto tra scalari}:
                    $\forall u \in V, \forall \lambda,\mu\in \mathbb{R}: (\lambda \cdot
                        \mu)u = \lambda \cdot (\mu \cdot u)$
              \item \textbf{Esistenza dell'elemento neutro}: $\exists 1 \in
                        \mathbb{R}:1\cdot v = v, \forall v\in V$
          \end{itemize}
\end{itemize}
\begin{esempio}
    Un esempio di spazio vettoriale è $V\in \mathbb{R}^n$
\end{esempio}
\begin{definizione} [\textbf{Prodotto scalare}]
    Sia $V$ uno spazio vettoriale, definiremo il \textbf{prodotto scalare} è un'operazione
    che a due elementi di $V$ associa un valore reale, $(\cdot, \cdot):V\times V
        \rightarrow \mathbb{R}$.Tale operazione soddisfa le seguenti proprietà:
    \begin{itemize}
        \item \textbf{Simmetrica}: $\forall u,v \in V, (u,v) = (v,u)$
        \item \textbf{Bi-lineare}: $\forall v_1,v_2,w \in V,\forall\alpha_1,
                  \alpha_2 \in \mathbb{R}:(\alpha_1v_1+\alpha_2v_2, w) =
                  \alpha_1(v_1, w) + \alpha_2(v_2, w)$
        \item $\forall v \in V, v\ne 0: (v,v) \ge 0$
    \end{itemize}
\end{definizione}
\begin{definizione} [\textbf{Norma di un vettore}]
    Sia $V$ uno spazio vettoriale, definiremo la \textbf{norma di un vettore} è
    un'operazione che a un elemento di $V$ associa un valore reale, $\|\cdot\|:
        V \rightarrow \mathbb{R}$. Tale operazione soddisfa le seguenti proprietà:
    \begin{itemize}
        \item \textbf{scalabilità}: $\forall v \in V,\forall \alpha \in \mathbb{R}:
                  \|\alpha v\| = |\alpha | \|v\|$
        \item \textbf{disuguaglianza triangolare}: $\forall v,w \in V: \|v+w\|
                  \le \|v\| + \|w\|$
        \item $\forall v \in V, v\ne 0: \|v\| > 0\implies \|0\| = 0$
    \end{itemize}
\end{definizione}
La definizione e le proprietà derivano dalla nota successiva
\begin{nota} [\textbf{Norma indotta dal prodotto scalare}]
    Sia $V$ uno spazio vettoriale in cui è definito un prodotto scalare allora è
    possibile calcolare la norma da $(\cdot, \cdot)$
    \begin{equation}
        \|v\| = \sqrt{(v,v)}  \equiv \|v\|_2
    \end{equation}
    Tale norma viene chiamata \textbf{norma indotta dal prodotto scalare }
\end{nota}
\begin{nota}
    Esistono diverse norme oltre alla $2$:
    \begin{itemize}
        \item $\|v\|_2 = \sqrt{\sum_{i=1}^{N}v_i^2} = \|v-0\|$
        \item $\|v\|_1 = \sum_{i=1}^{N}|v_i|$
        \item $\|v\|_\infty = \max_{i= 1\dots n}|v_i|$
    \end{itemize}
\end{nota}
La norma euclidea ($\|\cdot\|_2$) è utile per calcolare la distanza di vettori
perché $\|v\|_2$ calcola la distanza di $v$ da $0$, mentre $\|v-w\|_2$ calcola
la distanza tra $v$ e $w$.

Quindi dato un prodotto scalare possiamo sempre definire la norma, ma non possiamo
dire il contrario.
\begin{nota}
    La norma ha una proprietà indotta dalla sua definizione e dalle sue proprietà,
    $\forall v,w \in V$
    \begin{equation*}
        \|v\| -\|w\| \le \|v-w\|
    \end{equation*}
\end{nota}
Possiamo definire anche le norme per le matrici $V\in \mathbb{R}^{r\times c}$:
\begin{itemize}
    \item $\|A\|_F = \sqrt{\sum_{i,j} |a_{i,j}|^2}$
    \item $\|A\|_1 = \max_{i=1\dots r}{\sum_{j=1}^{c} |a_{i,j}|}$
    \item $\|A\|_\infty = \max_{j=1\dots c}{\sum_{i=1}^{r} |a_{i,j}|}$
\end{itemize}

Altre operazioni utili per gli spazi vettoriali, generalmente per vettori e matrici, è
la \textbf{trasposizione}.
\begin{equation}
    A=\left[\begin{array}{ccc}
            a & b & c \\
            d & e & f
        \end{array}\right]\in \mathbb{R}^{2\times3},  A^t = \left[\begin{array}{cc}
            a & d \\
            b & e \\
            c & f
        \end{array}\right]\in \mathbb{R}^{3\times2}
\end{equation}
Il problema di questa operazione è che non è \textbf{chiusa} ovvero non rimane
nello stesso spazio.

In aggiunta si ha \textbf{prodotto tra matrici e vettori}. Dati $A\in \mathbb{R}^{r\times c}$
e $v\in \mathbb{R}^{c}$ si ha:
\begin{equation*}
    Av = \left[\begin{array}{cccc}
            a_{11} & a_{12} & \cdots & a_{1c} \\
            a_{21} & a_{22} & \cdots & a_{2c} \\
            \vdots & \vdots & \dots  & \vdots \\
            a_{r1} & a_{r2} & \cdots & a_{rc}
        \end{array}\right] \left[\begin{array}{c}
            v_1 \\v_2\\\vdots\\v_c
        \end{array}\right] = \left[\begin{array}{c}
            a_{11}v_1+a_{12}v_2+\cdots a_{1c}v_c \\
            a_{21}v_1+a_{22}v_2+\cdots a_{2c}v_c \\
            \vdots                               \\
            a_{r1}v_1+a_{r2}v_2+\cdots a_{rc}v_c \\
        \end{array}\right]
\end{equation*}
Questa operazione è vincolata dal fatto che la matrice e il vettore devono
essere compatibili, le colonne della matrice devono essere uguali alle righe del
vettore. Inoltre abbiamo anche l'operazione di \textbf{prodotto matrice-matrice}.
Sia $A\in \mathbb{R}^{3\times 2}, B\in \mathbb{R}^{2\times 4}$
\begin{equation*}
    AB = \left[\begin{array}{cc}
            a & b \\
            c & d \\
            e & f
        \end{array}\right]\left[\begin{array}{cccc}
            g & h & i & l \\
            m & n & o & p \\
        \end{array}\right] = \left[\begin{array}{cccc}
            ag+bm & ah+bn & ai+bo & al+bp \\
            cg+dm & ch+dn & ci+do & cl+bp \\
            eg+fm & eh+fn & ei+fo & el+fp \\
        \end{array}\right]
\end{equation*}
Anche questa operazione è vincolante perché può essere effettuata solo quando la
prima matrice ha lo stesso numero di colonne uguale al numero di righe della
seconda. Inoltre la commutazione non sempre è fattibile perché potrebbero non
essere compatibili e in generale non è commutativa. La soluzione del prodotto 
può essere anticipato quando tra i fattori si hanno matrici particolari.

Supponiamo di avere $A\in \mathbb{R}^{3\times 3}$ sparsa e $B\in \mathbb{R}^{3\times3}$
\begin{equation*}
    \left[\begin{array}{ccc}
        1 & 0 & 0 \\
        0 & 0 & 1 \\
        0 & 1 & 0
    \end{array}\right]  \left[\begin{array}{ccc}
        a & b & c \\
        e & f & g \\
        h & i & l
    \end{array}\right] = \left[\begin{array}{ccc}
        a & b & c \\
        h & i & l \\
        e & f & g \\
    \end{array}\right] 
\end{equation*}
In questo caso si ha uno scambio della seconda riga con la terza.
\begin{equation*}
     \left[\begin{array}{ccc}
        a & b & c \\
        e & f & g \\
        h & i & l
    \end{array}\right] \left[\begin{array}{ccc}
        1 & 0 & 0 \\
        0 & 0 & 1 \\
        0 & 1 & 0
    \end{array}\right] = \left[\begin{array}{ccc}
        a & c & b \\
        e & g & f \\
        h & l & i \\
    \end{array}\right] 
\end{equation*}
In questo caso abbiamo fatto lo scambio della seconda colonna con la terza.

Queste matrici sono particolari perché vengono chiamate \textbf{matrici di permutazioni}
che permettono di scambiare delle righe o delle colonne. Queste sono utili per 
risolvere i sistemi lineari e permettono di velocizzare le operazioni di calcolo. 

Supponiamo di avere una nuova matrice
\begin{equation*}
    \left[\begin{array}{ccc}
        1 & 0 & 0 \\
        0 & 1 & 0 \\
        0 & 0 & 1
    \end{array}\right]  \left[\begin{array}{ccc}
        a & b & c \\
        d & e & f \\
        g & h & i
    \end{array}\right] = \left[\begin{array}{ccc}
        a & b & c \\
        2a+d & 2f+e & 2c+f \\
        g & h & i \\
    \end{array}\right] 
\end{equation*}

In aggiunta possiamo trasformare un sistema di equazioni lineari in un prodotto 
tra matrice e vettore ($Ax+b=0\equiv a_1x_1+a_2x_2+\dots a_n x_n = b$). Dove $a_i$
è il vettore dei coefficienti della variabile $x_i$. Questa rappresentazione dei
sistemi permette di risolverli facilmente utilizzando dei metodi specifici per le 
tipologie di matrici marticolari, fondamentale sarà quindi riconoscere la tipologia
di matrici.

Altre matrici particolari sono quelle \textbf{triangolari inferiori/superiori} che sono
utili per risolvere i sistemi lineari usando il metodo di sostituzioni.

\begin{esempio}
    Esempio di matrice triangolare inferiore
    \begin{equation*}
        \left[\begin{array}{ccc}
            1 & 0 & 0 \\
            2 & 1 & 0 \\
            1 & 1 & 1
        \end{array}\right] 
    \end{equation*}
\end{esempio}

\begin{esempio}
    Esempio di matrice triangolare superiore
    \begin{equation*}
        \left[\begin{array}{ccc}
            1 & 1 & 1 \\
            2 & 1 & 0 \\
            1 & 0 & 0 \\
        \end{array}\right] 
    \end{equation*}
\end{esempio}

Un'altra matrice utile è quella \textbf{simmetrica}, ovvero una matrice quadrata
tale che $a_{ij } = a_{ji}$.

Un altro tipo di matrice particolare è quella dei sistemi con moltissime variabili,
moltissime equazioni ciascuna con poche variabili, quindi si parlerà di \textbf{sistema 
lineare sparsi} (minore del $10\%$ delle entrate sono diverse da $0$). Importante sarà 
trovare un modo per salvare i valori delle matrici in modo efficiente. 

Un operazione utile per la risoluzione dei sistemi lineari è il \textbf{determinante},
ovvero $det(A): \mathbb{R}^{n\times n} \rightarrow \mathbb{R}$. Il determinante
è utile per scoprire la presenza di soluzioni e se sono uniche.

\section{Risoluzione di sistemi lineari al PC}
Per sviluppare un algoritmo di risoluzione del sistema lineare deve considerare 
tre fattori:
\begin{itemize}
    \item \textbf{memoria}: vogliamo un algoritmo che richieda un ridotto quantitativo 
    di memoria
    \item \textbf{velocità}: vogliamo un algoritmo molto veloce
    \item \textbf{precisione}: vogliamo ottenere una soluzione più vicina possibile
    a quella reale
\end{itemize}

Per salvare un array si può utilizzare i classici array della programmazione. Per 
quanto riguarda la matrice, si possono salvare in array secondo la filoosofia 
di \textbf{row major}. Alternativamente c'è la metodologia \textbf{column major}
che srotola la matrice rispetto la colonna.

Per salvare una matrice sparsa viene sempre salvata come collezione di tuple $\langle i,j,val\rangle$.
Con questa rappresentazione ogni qualvolta si effettua un'operazione tra matrici
sparse si trasformano in matrici normali e si applica l'operazione.

Per calcolare la precisione dell'algorimo che restituisce una soluzione floating 
point $\stackrel{\sim}{x}$ differente rispetto al valore corretto $x$, si usa 
l'errore relativo e la norma
\begin{equation*}
    \epsilon = \frac{\|x-\stackrel{\sim}{x}\|}{\|x\|}
\end{equation*}

\begin{teorema}
    Le norme di vettori sono equivalenti ossia esistono $c_1,c_2\in \mathbb{R}$ 
    tale che 
    \begin{equation*}
        c_1\|x\|_p < \|x\|_q < c_2\|x\|_p
    \end{equation*}
\end{teorema}
Il teorema ci dice che le norme a meno di alcuni fattori moltiplicativi restituiscono
gli stessi risultati.

Per calcolare la velocità di un algoritmo dobbiamo contare quante operazioni effettuiamo
e ragioniamo con la complessità asintotica.
\begin{esempio}
    Sia $(x,y) = \sum_{i=1}^n x_iy_i$ corrisponde ad effettuare $n$ prodotti e $n-1$ summe,
    quindi si ha $n+n-1 = 2n-1 \in \mathcal{O}(n)$ 
\end{esempio}

Introduciamo la velocità delle diverse operazioni:
\begin{itemize}
    \item \textbf{prodotto scalare}: $\mathcal{O}(n)$
    \item \textbf{prodotto matrice-vettore}: siano $A\in \mathbb{R}^{r\times c}$ e $v\in \mathbb{R}^c$
    allora $Av\in \mathcal{O} (rc) = \mathcal{O}(n^2)$ 
    \item \textbf{prodotto matrice-matrice}: siano $A,B\in \mathbb{R}^{n\times n}$
    allora $AB\in \mathcal{O} (nnn) = \mathcal{O} (n^3)$ 
    \item \textbf{determinate}: sia $A\in \mathbb{R}^{n\times n}$ allora $det(A) \in  \mathcal{O}(n!)$
\end{itemize}

Alla luce di questi ragionamenti si può osservare che se noi usassimo il calcolo
del determinante per capire se esistono soluzioni, noi non possiamo fidarci del 
risultato dal momento che ci sono errori e risulta complesso sapere se il risultato
vicino a $0$ è effettivamente $0$.

Per sapere se il sistema è complesso si userà il \textbf{numero di condizionamento}
calcolato su una matrice, se è alto allora il sistema è complesso da risolvere.
Il numero di condizionamento cresce al crescere della matrice, se si ha un'ordine di 
grandezza differente fra le entrate della matrice. Più il valore di condizionamento
è alto allora significa che saremo poco precisi.

\begin{esempio}
    Un esempio di matrice mal condizionata è quella di Hillbert.
    \begin{equation*}
        H_{ij}=\frac{1}{1+i+j}
    \end{equation*}
\end{esempio}

