\chapter{Metodi diretti}
Partiamo analizzando il caso più semplice, ovvero che il sistema ammette un unica
soluzione. L'assunzione principale è che le matrici abbiano determinante diverso
da $0$.
\section{Sostituzione all'indietro}
Il metodo della \textbf{sostituzione all'indietro} è il metodo più efficiente per
risolvere sistemi composti da una matrice quadrata triangolare superiore.
\begin{equation}
    \begin{cases}
        u_{11}x_1 + u_{12} x_2 + \dots + u_{1n} x_n= b_1 \\
        u_{22}x_2 + u_{23} x_3 + \dots + u_{2n} x_n= b_2 \\
        \vdots                                           \\
        u_{nn}x_n = b_n                                  \\
    \end{cases} \iff \left[\begin{array}{cccc}
            u_{11} & u_{12} & \dots  & u_{1n} \\
            0      & u_{22} & \dots  & u_{2n} \\
            \vdots & \ddots & \ddots & \vdots \\
            0      & \dots  & 0      & u_{nn}
        \end{array} \right] \cdot x = \left[\begin{array}{c}
            b_1    \\
            b_2    \\
            \vdots \\
            b_n
        \end{array}\right]
\end{equation}
A livello computazione è semplice testare se la matrice è compatibile con questo
metodo.

Una volta effettuata la verifica, il metodo coincide con il risolvere il sistema
per sostituzione.
\begin{nota}
    Bisogna stare attenti che la diagonale non contenga $0$ perché ogni volta
    si divide per un termine sulla diagonale.
\end{nota}

Sotto le ipotesi di matrice triangolare superiore possiamo calcolare facilmente
il determinante, basta calcolare il prodotto dei termini sulla diagonale, questo
ci dice che esiste una sola soluzione se il determinante è diverso da $0$
($\mathcal{O}(n)$). In aggiunta, questo ci assicura che la diagonale non contiene $0$.

La complessità dell'algoritmo sarà:
\begin{equation*}
    \#\_righe \cdot \#operazioni\_max\_riga = n \cdot (n - 1 + n - 1 + 1) = n
    \cdot (2n - 1) = \mathcal{O}(n^2)
\end{equation*}
\begin{nota}
    Dove il numero massimo di operazioni per riga è $n - 1$ differenze, $n - 1$
    prodotti e $1$ divisione perché nella prima riga si ha:
    \begin{equation*}
        x_1 = \frac{(b_1 - u_{12} x_2 - u_{13}x_3 - \dots - x_{1n} x_n)}{u_{11}}
    \end{equation*}
\end{nota}
Ogni singola operazione per riga si può ricavare iterativamente nel seguente modo
\begin{equation*}
    x_{n-i} = \frac{(b_i-(u_{n-i},x))}{u_{ii}}
\end{equation*}
Con $x$ inizializzato a tutti $0$ tranne la cella $n$ che sarà inizializzata a:
\begin{equation*}
    x_n = \frac{b_n}{u_{nn}}
\end{equation*}
Mentre $i$ è inizializzato a $1$, ad ogni iterazione si utilizza il vettore $x$
aggiornato nell'iterazione precedente. Al termine si ottiene una soluzione per il
sistema.
\begin{nota}
    Questo metodo può essere utilizzato anche per i sistemi lineari descritti da
    una matrice triangolare inferiore. In questo caso prima sarà necessario
    trasformare la matrice in una matrice triangolare superiore. Questo si può
    fare riordinando le righe della matrice e specchiando la matrice la colonna
    centrale. Lo stesso ragionamento si può applicare anche per matrici diagonali
    rispetto alla antidiagonale.
\end{nota}
\begin{nota}
    Ricorda che quando si effettua un permutazione delle righe il sistema rimane
    invariato, al contrario se effettuassi una permutazione delle colonne allora
    sto cambiando le incognite, perciò dovrei cambiare anche il vettore delle
    incognite se io volessi mantenere lo stesso sistema.
\end{nota}
\section{Trasformazioni di Gau$\Beta$}
Per risolvere sistemi a cui sono associate matrici che non sono triangolari,
come quella riportata di seguito, non possiamo più pensare all'algoritmo di
sostituzione all'indietro.
\begin{equation}
    \begin{cases}
        a_{11}x_1 + a_{12} x_2 + \dots + a_{1n} x_n= b_1 \\
        a_{21}x_1 + a_{22} x_2 + \dots + a_{2n} x_n= b_2 \\
        \vdots                                           \\
        a_{n1}x_1 + a_{n2} x_2 + \dots + a_{nn} x_n= b_n \\
    \end{cases}
\end{equation}

Per sistemi generici di $n$ equazioni in $n$ incognite i metodi di risoluzione
sono diversi. L'algoritmo di eliminazione di Gauss si basa sul fatto che, preso
un sistema lineare è possibile modificare le sue equazioni in modo da ottenere
un sistema lineare equivalente, ossia che ha la stessa soluzione del sistema di
partenza. Questo si può fare utilizzando le seguenti trasformazioni:
\begin{itemize}
    \item \textbf{Scambiare le righe}
    \item \textbf{Riduzione}: Sostituire una riga con una combinazione lineare di
          se stessa con un'altra
\end{itemize}
Successivamente si eseguirà sul sistema equivalente il metodo di risoluzione per
sostituzione all'indietro.

Per effettuare riduzione della matrice associata al sistema ad una matrice
triangolare superiore, applichiamo una serie di trasformazioni in modo da azzerare
i coefficienti al di sotto della diagonale. L'applicazioni delle trasformazioni
deve essere effettuata sulle righe della \textbf{augmented matrix form}, ovvero
la matrice del sistema con l'aggiunta del vettore dei termini noti a destra dei
coefficienti delle incognite.
\begin{definizione}[\textbf{Pivot}]
    Il termine che si sceglie per procedere con la sostituzione di tutte le
    equazioni del sistema lineare viene chiamato \textbf{pivot}.
\end{definizione}
\begin{nota}
    Sotto l'ipotesi di sistema risolubile (determinante non nullo) allora il metodo
    di Gau$\Beta$ convergerà ad una matrice triangolare superiore. Quindi non
    rischio di sostituire una riga con un coefficiente moltiplicatore di un'altra
    riga avente denominatore nullo. Quindi, quando abbiamo una riga che ha il
    coefficiente sulla diagonale pari a $0$ allora siamo sicuri che troveremo
    un'altra riga con un coefficiente non nullo.
\end{nota}
\begin{osservazione}
    Durante tutto il procedimento osserviamo che la scelta del pivot e quindi
    del moltiplicatore dipende solamente dalla matrice A e non dal termine noto.
    Il termine noto viene sempre e solo aggiornato in modo da ottenere un sistema
    lineare equivalente!
\end{osservazione}

Calcoliamo ora la complessità di tale algoritmo. Il peggiore dei casi è quando
effettuiamo la prima riduzione. Nello specifico, per essa dobbiamo calcolare:
\begin{equation*}
    R_i = R_i - \frac{a_{11}\cdot a_{a_i1}}{a_{11}} R_1
\end{equation*}
Queste operazioni sono $n + 1$ differenze, $n + 1$ prodotti e $1$ divisione. Quindi
per una riga sono necessarie $1 + n + 1 + n + 1 = 2 n + 3 =\mathcal{O}(n)$.
Questo deve essere ripetuto per $n - 1$ volte per azzerare la prima colonna,
in termini di complessità asintotica $\mathcal{O}(n^2)$. Dal momento che dobbiamo
ridurre un totale di $n-1$ colonne quindi si ha $\mathcal{O}(n^2\cdot n) = \mathcal{O}(n^3)$.
Attualmente non stiamo risolvendo il sistema, bensì lo stiamo solo trasformando,
per anche risolverlo impiegheremo  $\mathcal{O}(n^3+ n^2) =\mathcal{O}(n^3)$.

La scelta su da quale equazione partire porta ad avere cambiamenti sugli errori
ottenuti dal calcolo della soluzione dovuta all'aritmetica floating point. Quindi
l'obiettivo sarà di ridurre al minimo l'errore, sapendo che l'operazione problematica
è la somma/differenza soprattutto quando effettuiamo il calcolo $x + y$ dove $y
    \sim -x$. Quindi possiamo mitigare questi problemi utilizzando il \textbf{pivoting},
ovvero scegliere come pivot quello in valore assoluto maggiore presente sulla
colonna da ridurre.

Esistono due strategie per trovare il miglior pivot:
\begin{itemize}
    \item \textbf{Pivoting Parziale}: si cerca fra i coefficienti della matrice
          nella colonna che vogliamo eliminare il termine che ha il valore
          assoluto più alto.
    \item \textbf{Pivoting Totale}:  si cerca fra tutti i coefficienti della
          sotto matrice il termine più grande in modulo. Una volta scelto il
          coefficiente è necessario fare anche uno scambio di colonne in modo
          tale che quello sia il termine più a destra, altrimenti, andando
          avanti con l'eliminazione di Gau$\beta$, si perderebbe la struttura
          risultante di triangolare superiore.
\end{itemize}

Si sceglie il maggiore perché supponendo di essere in questo esempio:
$$a_{21} - \frac{a_{21}}{a_11} \ a_{22}-\frac{a_{21}}{a_11}$$
Sapendo che $a_{21}=0$ allora sostituisco direttamente con $0$ senza effettuare
la sottrazione (non compiendo errori) e se si sceglie come pivot il massimo allora
$\frac{a_{21}}{a_11}$ sarà piccolo quindi si compie un errore minore nel calcolo
di $a_{22}-\frac{a_{21}}{a_11}$. Si può ottimizzare ancora meglio effettuando il
\textbf{pivoting totale}.
\begin{table}[!ht]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{Pivot Totale} & \textbf{Casuale} & \textbf{Pivot Più Piccolo} \\
        $3.2138 e^{-14}$      & $1.0214 e^{-12}$ & $6.9658 e^{-11}$           \\
        \hline
    \end{tabular}
\end{table}

Utilizzando il metodo di Gau$\beta$ ha un problema, quando dobbiamo risolvere i
seguenti sistemi che presentano la stessa matrice ma con termini noti diversi:
\begin{equation*}
    Ax=b_1 \quad Ax=b_2 \quad Ax=b_3
\end{equation*}
In questa situazione dobbiamo risolvere tre volte lo stesso sistema in quanto
non si tiene memoria delle operazioni che sono state effettuate.

Sapendo che i termini noti sono indipendenti dalla scelta del pivoting allora
dobbiamo trovare un modo per aggiornare $b_i$ per evitare di aggiornare ogni
volta $A$.
\begin{osservazione}
    Risolvere più volte questo sistema variando $b_i$ coincide con il calcolo
    dell'inversa. Dove $x$ corrisponde alla colonna $i$ dell'inversa con il
    vettore di termini noti $b_i$, dove $b_i$ è il vettore di tutti $0$ tranne
    un $1$ nella posizione $i$-esima.
\end{osservazione}
Per risolvere questo problema uso la decomposizione LU. Ovvero dato un sistema
$Ax=b$, l'idea è di riscrivere la matrice $A$ come il prodotto di $L$ e $U$ ($A
    = L \cdot U$) che rappresentano rispettivamente una matrice triangolare
inferiore e una matrice triangolare superiore. Quindi si risolverà il sistema $L
    \cdot U \cdot x = b$, quando dobbiamo cambiare $b$ basta sostituire $b$ e le
matrici $L$ e $U$ rimarranno uguali.

Per risolvere $L\cdot U \cdot x =b$, prima si risolve $L\cdot y =b$ con una
\textbf{soluzione in avanti} $\mathcal{O}(n^2)$, poi si risolve $U \cdot x = y$
con una \textbf{soluzione all'indietro} $\mathcal{O}(n^2)$.
\begin{teorema}
    Sia $A \in \mathbb{R}^{n\times n}$ una matrice non singolare (determinante
    non nullo), allora esistono $L,U \in \mathbb{R}^{n\times n}$ tali che $A =
        L \cdot U$.
    Dove la matrice $U$ è definita come:
    \begin{equation*}
        U=\left[\begin{array}{ccc}
                a_{11} & a_{12}  & a_{13}   \\
                0      & a_{22}' & a_{23}'  \\
                0      & 0       & a_{33}'' \\
            \end{array}\right]
    \end{equation*}
    Mentre la matrice $L$ è definita come:
    \begin{equation*}
        L=\left[\begin{array}{ccc}
                1      & 0      & 0 \\
                l_{21} & 1      & 0 \\
                l_{31} & l_{32} & 1 \\
            \end{array}\right]
    \end{equation*}
\end{teorema}
Per calcolare $L,U$ ci si impiega $\mathcal{O}(n^3)$ e generalmente $L$ viene
salvata in $U$.
\begin{nota}
    Possiamo pensare di risolvere $Ax=b$ in $x= A^{-1}b$, il problema è che per
    trovare $A^{-1}$ servono $\mathcal{0}(n^4)$ operazioni in cui si accumulano
    errori. Inoltre per trovare l'inversa risolviamo più sistemi.
\end{nota}
\begin{nota}
    Un possibile utilizzo della decomposizione $LU$ è quello di calcolare il
    determinante della matrice $A$. Questo può essere fatto nel seguente modo:
    \begin{equation}
        det(A) = det(LU) = det(L)det(U)
    \end{equation}
    dal momento che $L$ e $U$ sono matrici triangolari, allora il determinante è
    il prodotto dei coefficienti sulla diagonale. Inoltre $L$ ha solamente il
    valore $1$ sulla diagonale, quindi $det(L) = 1$ mentre il $det(U) = u_{11}
        \dots u_{nn}$. Quindi per calcolare il determinante costa $\mathcal{O}(n^3)$
    dalla decomposizione.
\end{nota}
\section{Decomposizione PALU}
La decomposizione PALU permette di risolvere un insieme di sistemi lineari in
cui varia solamente il termine noto.
\begin{teorema}
    Sia $A \in \mathbb{R}^{n\times n}$ una matrice quadrata tale che $det(A) \neq 0$,
    allora esistono tre matrici $P,L,U$ tali che:
    \begin{equation}
        PA=LU
    \end{equation}
    dove $P$ è una matrice di permutazione, $L$ è una matrice triangolare inferiore
    con diagonale unitaria e $U$ è una matrice triangolare superiore.
\end{teorema}
La matrice di permutazione $P$ mi permette di effettuare la scelta del pivot migliore.
\begin{nota}
    Anche per la decomposizione $PALU$, il determinante è semplice calcolare
    il determinante della matrice $A$, nel seguente modo:
    \begin{equation*}
        det(PA) = det(LU)
    \end{equation*}
    quindi $det(P) \cdot det(A) = det(L) \cdot det(U)$, dal momento che $P$ è
    una matrice di permutazione allora si può dimostrare che il determinante della
    matrice di permutazione si può calcolare come:
    \begin{equation*}
        det(P) = (-1)^{\#\_scambi}
    \end{equation*}
    Quindi $(-1)^{\#\_scambi}det(A) = det(L)det(U)$ ovvero:
    \begin{equation*}
        det(A) = \frac{det(L)det(U)}{(-1)^{\#\_scambi}}
    \end{equation*}
\end{nota}
\section{Decomposizione di Cholesky}
Se la matrice $A$ che stiamo considerando, presenta delle caratteristiche
particolari, può essere fattorizzata utilizzando una strategia differente
rispetto alla decomposizione $PALU$, ovvero la decomposizione di \textbf{Cholesky}.

Si può applicare questa decomposizione su matrici che rispettano queste ipotesi:
\begin{itemize}
    \item $A$ simmetrica, $A = A^t$.
    \item $A$ è definita positiva, $y^t A y >0, \forall y \in \mathbb{R}, y \neq \vec{0}$,
          oppure tutti i suoi autovalori sono positivi (non si ammettono
          autovalori nulli).
\end{itemize}
\begin{teorema}
    Sia $A \in \mathbb{R}^{n\times n}$ una matrice simmetrica e definita positiva,
    allora esiste una matrice $R \in \mathbb{R}^{n\times n}$ tale che:
    \begin{equation}
        A = R^t R
    \end{equation}
\end{teorema}
La matrice $R$ è una matrice triangolare superiore.
\begin{dimostrazione}
    Dimostreremo questo teorema in modo costruttivo, ossia diremo come costruire
    la sola matrice $R$ che ci serve a partire dai coefficienti della matrice $A$.

    Data la matrice $A$ definita come:
    \begin{equation*}
        A=\left[\begin{array}{cccc}
                a_{11} & a_{12} & \dots  & a_{1n} \\
                a_{21} & a_{22} & \dots  & a_{2n} \\
                \vdots & \vdots & \ddots & \vdots \\
                a_{n1} & a_{n2} & \dots  & a_{nn} \\
            \end{array}\right]
    \end{equation*}
    che rispetta le ipotesi della decomposizione di Cholesky, supponiamo di
    suddividere la matrice in queste regioni.
    \begin{equation*}
        A=\left[\begin{array}{c|c}
                \alpha       & \textbf{a}_1^t \\
                \textbf{a}_1 & A_\ast         \\
            \end{array}\right]
    \end{equation*}
    dove abbiamo definito:
    \begin{equation*}
        \textbf{a}_1 = \left[\begin{array}{c}
                a_{12} \\
                \vdots \\
                a_{1n} \\
            \end{array}\right] \quad A_{1, 1} = \left[
            \begin{array}{ccc}
                a_{2, 2} & \dots  & a_{2, n} \\
                \dots    & \vdots & \dots    \\
                a_{n, 2} & \dots  & a_{n, n}
            \end{array} \right]
    \end{equation*}
    In questo modo dobbiamo cercare i coefficienti della matrice $R$ che permetta
    di ricavare ciascun coefficiente della matrice $A$ che si sta selezionando.

    Quindi la matrice $R$ può essere definita come:
    \begin{equation*}
        A=\left[\begin{array}{c|c}
                \rho          & \underline{r} \\
                \hline
                \underline{0} & R_\ast        \\
            \end{array}\right]
    \end{equation*}
    In questo modo bisogna trovare $\rho, r$ che permetta di ottenere secondo la
    relazione $A = R^tR$ il coefficiente $a_{11}$, successivamente si reitera lo
    stesso procedimento per tutti gli altri coefficienti ottenendo sotto-matrice
    $R_\ast$.

    Data la matrice $A$, la quale rispetta le ipotesi della decomposizione, allora
    sapendo che essa è simmetrica possiamo rappresentarla come segue:
    \begin{equation*}
        A=\left[\begin{array}{c|ccc}
                a_{11}            & \underline{a_1} \\
                \hline
                \underline{a_1}^t & A_\ast
            \end{array}\right]
    \end{equation*}
    Quindi possiamo dire che:
    \begin{equation*}
        \left[\begin{array}{c|ccc}
                a_{11}            & \underline{a_1} \\
                \hline
                \underline{a_1}^t & A_\ast
            \end{array}\right] = A = R^tR= \left[\begin{array}{c|c}
                \rho            & \underline{0}^t \\
                \hline
                \underline{r}^t & R_\ast^t        \\
            \end{array}\right]\left[\begin{array}{c|c}
                \rho          & \underline{r} \\
                \hline
                \underline{0} & R_\ast        \\
            \end{array}\right]=\left[\begin{array}{c|c}
                \rho^2                   & \rho \cdot\underline{r}                      \\
                \hline
                \underline{r}^t\cdot\rho & \underline{r}^t\underline{r}+R_\ast^t R_\ast \\
            \end{array}\right]
    \end{equation*}
    Dato che il vettore $r \in \mathbb{R}^n$ allora possiamo dire che:
    \begin{equation*}
        \underline{r}^t\underline{r} \in \mathbb{R}^{n\times n}
    \end{equation*}
    Dalla generazione del prodotto $R^tR$ allora possiamo dire che:
    \begin{equation*}
        \begin{cases}
            \rho^2 = a_{11}                                                                     \\
            \underline{r} = \rho \underline{a}_1 & \text{che vale sia per riga sia per colonna} \\
            \underline{r}^t\underline{r}+R_\ast^t R_\ast = A_\ast
        \end{cases}
    \end{equation*}
    Risolvendo questo sistema le incognite $\rho, r, R_\ast$:
    \begin{equation*}
        \begin{cases}
            a_{11} = \rho ^2                    \\
            \underline{a}_1= \rho \underline{r} \\
            A_\ast = \underline{r}^t\underline{r}+R_\ast^t R_\ast
        \end{cases}\iff \begin{cases}
            \rho = +\sqrt{a_{11}}                \\
            \underline{r}= \rho  \underline{a}_1 \\
            R_\ast^t R_\ast = A_\ast - \frac{1}{a_{11}} \underline{a}_1^t \underline{a}_1
        \end{cases}
    \end{equation*}
    Dato che una delle ipotesi su cui si basa la decomposizione di Cholesky è che la
    matrice sia definita positiva, allora siamo sicuri ogni coefficiente sulla
    diagonale sarà sempre $> 0$, permettendoci di calcolare $\rho$.
\end{dimostrazione}

Questo sistema è ricorsivo e il problema è che bisogna reiterare il procedimento
per ottenere gli altri coefficienti, quindi è necessario che $R_\ast ^t R_\ast$
sia simmetrica e definita positiva. Proviamo a dimostrarlo, chiamiamo $B$ la
matrice definita come $R_\ast ^t R_\ast$, quindi $B = A_\ast - \frac{1}{a_{11}}
    \underline{a}_1^t \underline{a}_1$.
\begin{itemize}
    \item Dimostriamo che la matrice B sia simmetrica:
          \begin{equation*}
              B^t = \left(A_\ast - \frac{1}{a_{11}} \underline{a}_1^t
              \underline{a}_1\right)^t= A_\ast - \frac{1}{a_{11}} (\underline{a}_1^t
              \underline{a}_1)^t =A_\ast - \frac{1}{a_{11}} \underline{a}_1^t
              \underline{a}_1 = B
          \end{equation*}
    \item Dimostriamo che sia definita positiva:
          \begin{equation*}
              0<\underline{y}^t A \underline{y} = \left[\begin{array}{cc}
                      \eta & \underline{y_1}
                  \end{array}\right]\left[\begin{array}{c|ccc}
                      a_{11}            & \underline{a_1} \\
                      \hline
                      \underline{a_1}^t & A_\ast
                  \end{array}\right]\left[\begin{array}{c}
                      \eta \\
                      \underline{y_1}
                  \end{array}\right] = \left[\begin{array}{cc}
                      \eta & \underline{y_1}
                  \end{array}\right]\left[\begin{array}{c}
                      \eta a_{11}+\underline{a_1}\underline{y}_1^t \\
                      \eta\underline{a_1}^t + A_\ast \underline{y}_1^t
                  \end{array}\right] =
          \end{equation*}
          \begin{equation*}
              = \eta^2 a_{11}+\eta\underline{a_1}\underline{y}_1^t+\eta
              \underline{y}_1\underline{a_1}^t + \underline{y}_1 A_\ast
              \underline{y}_1^t = \eta^2 a_{11}+2\eta\underline{a_1}\underline{y}_1^t +
              \underline{y}_1 A_\ast \underline{y}_1^t>0
          \end{equation*}
          Quindi equivale ad una disequazione di secondo grado in $\eta$, quindi
          bisogna controllare che $\Delta < 0\implies b^2-4ac<0$, questo significa:
          \begin{equation*}
              \Delta =(2\underline{a_1}\underline{y}_1^t)^2 -4a_{11}
              \underline{y}_1 A_\ast \underline{y}_1^t = \not 4(\underline{a_1}
              \underline{y}_1^t)^2 -\not 4 a_{11} \underline{y}_1 A_\ast \underline{y}_1^t
              < 0\iff
          \end{equation*}
          \begin{equation*}
              \iff \underline{y}_1 A_\ast \underline{y}_1^t - \frac{1}{a_{11}}
              (\underline{a_1}\underline{y}_1^t)^2>0
          \end{equation*}
          Ma ripetendo la matrice $B$ e per controllare che sia definita positiva
          è esattamente:
          \begin{equation*}
              \underline{y}B\underline{y}^t=\underline{y}A_\ast\underline{y}^t -
              \frac{1}{a_{11}} \underline{y}\underline{a}_1^t \underline{a}_1\underline{y}^t >0
          \end{equation*}
          Che è esattamente quanto trovato precedentemente.
\end{itemize}
\section{Perturbazione dei dati}
Fino a questo momento ci siamo concentrati sul trovare la soluzione che meglio
approssima la soluzione esatta di un generico sistema lineare $Ax=b$. Ora vogliamo
analizzare come varia la soluzione approssimata $x_h$ al variare dei dati, ossia
se la matrice $A$ e il vettore $b$ vengono modificati.
\subsection{Perturbazione di b}
Fissiamo $A$ e consideriamo i seguenti sistemi lineari:
\begin{equation*}
    Ax=b \quad \text{e} \quad Ax=b+\delta b
\end{equation*}
In modo tale che $\|\delta b\|$ sia piccola. Sia $\stackrel{\sim }{x}$ la soluzione
del sistema $Ax=b$, vogliamo scoprire come $\stackrel{\sim}{x}$ si modifica nel
sistema perturbato. Possiamo osservare innanzitutto che:
\begin{equation}
    A\stackrel{\sim}{x}=b \implies \|A\stackrel{\sim}{x}\|=\|b\|\implies \|A\|\|
    \stackrel{\sim}{x}\|\geq \|b\|\implies\frac{1}{\|\stackrel{\sim}{x}\|} \leq \frac{\|A\|}{\|b\|}
\end{equation}
Con quanto detto, dal momento che $\stackrel{\sim}{x}$ è una soluzione del sistema
allora $det(A) \neq 0$ quindi la matrice è invertibile, di conseguenza:
\begin{equation}
    Ax = b + \delta b \implies A^{-1} Ax = A^{-1}(b + \delta b) \implies A^{-1}
    Ax = A^{-1} b + A^{-1} \delta b
\end{equation}
Dal momento che per un sistema in cui esiste una soluzione allora si ha che
$\stackrel{\sim}{x}= A^{-1}b$ allora possiamo dire che:
\begin{equation}
    A^{-1}Ax = A^{-1}b+A^{-1}\delta b\implies x = \stackrel{\sim}{x}+A^{-1}\delta
    b\implies x - \stackrel{\sim}{x} = A^{-1}\delta b
\end{equation}
Anche qui possiamo passare alle norme e ottenere i seguenti risultati:
\begin{equation}
    \|x - \stackrel{\sim}{x} \| = \|A^{-1}\delta b\|
\end{equation}
utilizzando le proprietà delle norme si ottiene:
\begin{equation}
    \|x - \stackrel{\sim}{x} \| \leq \|A^{-1}\|\|\delta b\|
\end{equation}
Unendo quanto ottenuto moltiplicando a sinistra per le norme delle soluzioni e a
destra per le norme dei termini dei sistemi, otteniamo la seguente disequazione:
\begin{equation}
    \frac{\|x - \stackrel{\sim}{x}\|}{\|\stackrel{\sim}{x}\|} \leq \frac{\|A\|}{\|b\|}
    \|A^{-1}\|\|\delta b\|
\end{equation}
Quanto scritto fino a questo momento può essere generalizzato per qualunque norma.
Dato che il nostro obiettivo è quello di capire la relazione tra il \textbf{numero
    di condizionamento} e la soluzione del sistema, usiamo la norma 2.
\begin{equation}
    \begin{aligned}
        \frac{\|x - \stackrel{\sim}{x} \|}{\|\stackrel{\sim}{x}\|} & \leq \frac{\|A\|}{\|b\|} \|A^{-1}\|\|\delta b\| \implies \\
        \frac{\|x - \stackrel{\sim}{x} \|}{\|\stackrel{\sim}{x}\|} & \leq \|A\| \|A^{-1}\|\frac{\|\delta b\|}{\|b\|} \implies \\
        \frac{\|x - \stackrel{\sim}{x} \|}{\|\stackrel{\sim}{x}\|} & \leq \|A\| \|A^{-1}\|\frac{\|\delta b\|}{\|b\|} \implies \\
        \frac{\|x - \stackrel{\sim}{x} \|}{\|\stackrel{\sim}{x}\|} & \leq  cond(A)\frac{\|\delta b\|}{\|b\|}
    \end{aligned}
\end{equation}
Questo perché possiamo scrivere il numero di condizionamento come:
\begin{equation*}
    \|A^{-1}\|_2\cdot \|A\|_2 = \frac{|\lambda_{\max}|}{|\lambda_{\min}|} = cond(A)
\end{equation*}
Nella disequazione si può notare come a sinistra abbiamo una sorta di \textit{errore
    relativo} tra la soluzione del sistema lineare originale e quella del sistema
lineare perturbato. A destra abbiamo la misura dell'entità di perturbazione e il
numero di condizionamento.

Data una piccola perturbazione e facendo variare $A$, dall'ultima disequazione
possiamo notare che più grande il condizionamento, più la soluzione del sistema
perturbato sarà diversa dal sistema originale. Questo offre informazioni in merito
al cambiamento delle soluzioni, più precisamente maggiore sarà il condizionamento,
più la matrice è sensibile al cambiamento del termine di destra e maggiore
sarà il cambiamento della soluzione.
\subsection{Perturbazione di A}
Consideriamo ora il caso in cui la matrice $A \in \mathbb{R}^{n \times n}$ viene
perturbata. Fissiamo il termine noto $b \in \mathbb{R}^n$ e consideriamo i seguenti
sistemi lineari:
\begin{equation*}
    Ax = b \quad \text{e} \quad (A + E)x = b
\end{equation*}
Sia $\stackrel{\sim}{x}$ la soluzione del sistema originale, vogliamo capire come
varia la soluzione applicando la perturbazione.

Il problema principale di questa casistica è dovuto al fatto che, sommando $A + E$
si introduce il rischio che il determinante si annulli ($det(A + E) = 0$) facendo
quindi diventare il sistema non risolvibile. Per evitare questo problema si può
sfruttare il seguente teorema.
\begin{teorema}
    Sia $A$ una matrice non singolare e sia $E$ una matrice tale che:
    \begin{equation*}
        \| A^{-1}E \| < 1
    \end{equation*}
    allora la matrice $A + E$ è non singolare.
\end{teorema}
Questo permette di scegliere la matrice $E$ in modo tale da preservare la
risolubilità del sistema.
\begin{nota}
    Nel teorema precedente si è utilizzata la definizione per una generica norma,
    dal momento che ci vogliamo ricondurre al numero di condizionamento allora
    useremo la norma $2$.
\end{nota}

Stimiamo come variano le soluzioni, partendo dall'ipotesi che $A$ sia invertibile
e $\stackrel{\sim}{x}$ è la soluzione del sistema originale.
\begin{equation*}
    (A + E)x = b \implies A^{-1}(A + E)x = A^{-1}b \implies x + A^{-1} Ex =
    \stackrel{\sim}{x} \implies x - \stackrel{\sim}{x} = -A^{-1} Ex
\end{equation*}
Successivamente passiamo alle norme e otteniamo:
\begin{equation*}
    \begin{aligned}
        \|x - \stackrel{\sim}{x}\| = \|-A^{-1}Ex\| & \implies \|x - \stackrel{\sim}{x}\|
        = |-1|\|A^{-1}Ex\| \implies                                                      \\
        \|x - \stackrel{\sim}{x}\| = \|A^{-1}Ex\|  & \implies \|x - \stackrel{\sim}{x}\|
        \leq \|A^{-1}E\|\|x\| \implies                                                   \\
        \frac{\|x - \stackrel{\sim}{x}\|}{\|x\|}\leq\|A^{-1}E\|
    \end{aligned}
\end{equation*}
Come effettuato per la perturbazione di $b$, vogliamo avere al denominatore
la norma della soluzione del sistema non perturbato ($\|\stackrel{\sim}{x}\|$).
Sia $\rho:=\|A^{-1}E\|$ allora:
\begin{equation}
    \frac{\|x - \stackrel{\sim}{x}\|}{\|x\|} \leq \rho \implies \rho \|x\|\geq
    \|x - \stackrel{\sim}{x}\| \implies \rho\|x\| \geq \|x\| - \|\stackrel{\sim}{x}\|
    \implies \|\stackrel{\sim}{x}\| \geq (1-\rho) \|x\| \implies \frac{1}{\|
        \stackrel{\sim}{x}\|} \leq \frac{1}{(1-\rho)\|x\|}
\end{equation}
Successivamente, moltiplicando entrambi i membri per $\|x-\stackrel{\sim}{x}\|$
si ottiene:
\begin{equation*}
    \frac{\|x - \stackrel{\sim}{x}\|}{\|\stackrel{\sim}{x}\|} \leq \frac{\|x -
        \stackrel{\sim}{x}\|}{(1 - \rho)\|x\|}
\end{equation*}
Successivamente, sapendo che $\rho = \frac{\|x - \stackrel{\sim}{x}\|}{\|x\|}$ allora:
\begin{equation*}
    \frac{\|x - \stackrel{\sim}{x}\|}{\|\stackrel{\sim}{x}\|} \leq \frac{\|x -
        \stackrel{\sim}{x}\|}{(1 - \rho)\|x\|} \implies \frac{\|x - \stackrel{\sim}{x}\|}
    {\|\stackrel{\sim}{x}\|} \leq \frac{\rho}{(1-\rho)}
\end{equation*}
Sapendo che $\rho$ è molto piccolo e positivo, perché deve valere il teorema
introdotto precedentemente, quindi $\rho = \|A^{-1}E\| < 1$, allora vale la seguente
approssimazione $\frac{\rho}{(1-\rho)} \approx \rho$.

Successivamente si possono espandere i calcoli per ottenere il numero di condizionamento.
\begin{equation*}
    \begin{aligned}
        \frac{\|x - \stackrel{\sim}{x}\|}{\|\stackrel{\sim}{x}\|} \leq \|A^{-1}E\|
         & \implies \frac{\|x - \stackrel{\sim}{x}\|}{\|\stackrel{\sim}{x}\|} \leq \|A^{-1}\|\|E\| \implies \\
        \frac{\|x - \stackrel{\sim}{x}\|}{\|\stackrel{\sim}{x}\|} \leq \|A^{-1}\|\|E\|\frac{\|A\|}{\|A\|} 
        & \implies \frac{\|x - \stackrel{\sim}{x}\|}{\|\stackrel{\sim}{x}\|} \leq \|A^{-1}\|\|A\|\frac{\|E\|}{\|A\|} \implies \\
        & \implies \frac{\|x - \stackrel{\sim}{x}\|}{\|\stackrel{\sim}{x}\|} \leq cond(A)\frac{\|E\|}{\|A\|}
    \end{aligned}
\end{equation*}
Ovviamente il passaggio al numero di condizionamento vale per la norma $2$. La
soluzione del sistema perturbato dipende dal numero di condizionamento di $A$ e
da quanto è perturbato il sistema.