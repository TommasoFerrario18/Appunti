\chapter{Autovalori e autovalori}
\begin{definizione} [\textbf{Autovalore e autovettore}]
    Data una matrice $A \in \mathbb{R}^{n\times n}$ un \textbf{autovalore} di una
    matrice e l'\textbf{autovettore} associato ad esso, sono una coppia $(\lambda,
        \vec{v}_\lambda)\in \mathbb{C}\times\mathbb{C}^{n}$ tale che vale la
    seguente identità:
    \begin{equation*}
        A\vec{v}_\lambda = \lambda \vec{v}_\lambda
    \end{equation*}
\end{definizione}
In generale una matrice A di dimensione n ha n autovalori/autovettori. Trovare
lo \textbf{spettro}, ossia tutti gli n autovalori/autovettori, è una procedura
onerosa in termini di operazioni macchina.
\section{Localizzazione autovalori}
Dato che gli autovalori $\lambda$ di una matrice $A$ sono numeri complessi, dovremmo
lavorare sul piano complesso e non semplicemente sulla retta reale.

Una prima stima sulla posizione degli autovalori è fornita dalla norma della
matrice $A$.Infatti vale il seguente teorema.
\begin{teorema}
    Sia $\|\cdot\|$ una norma di matrici e sia $A$ una matrice quadrata, allora
    sappiamo che:
    \begin{equation*}
        |\lambda_{\max}| \leq \|A\|
    \end{equation*}
\end{teorema}
Possiamo studiare questo teorema considerando che la norma di una matrice è
definita come:
\begin{equation*}
    \|A\| = \max_{\vec{x}_\lambda \in \mathbb{R}^n} \frac{\|A\vec{x}_\lambda\|}{\|\vec{x}_\lambda\|}
\end{equation*}
Dato che vogliamo verificare la seguente identità:
\begin{equation*}
    A\vec{v}_\lambda = \lambda \vec{v}_\lambda
\end{equation*}
di sicuro, vogliamo che la norma di $A \vec{v}_\lambda$ e $\lambda \vec{v}_\lambda$
siano uguali, ovvero:
\begin{equation*}
    \|A\vec{v}_\lambda\| = \|\lambda \vec{v}_\lambda\|
\end{equation*}
essendo $\lambda$ uno scalare possiamo scrivere:
\begin{equation*}
    \|A\vec{v}_\lambda\| = |\lambda| \|\vec{v}_\lambda\|
\end{equation*}
A questo punto, se dividiamo il termine a sinistra per $\|\vec{v}_\lambda\|$ otteniamo:
\begin{equation*}
    \|\vec{v}_\lambda\| \frac{\|A\vec{v}_\lambda\|}{\|\vec{v}_\lambda\|} = |\lambda| \|\vec{v}_\lambda\|
    \leq \left(\max_{\vec{x}_\lambda \in \mathbb{R}^n} \frac{\|A\vec{x}_\lambda\|}{\|\vec{x}_\lambda\|}\right) \cdot \|\vec{v}_\lambda\|
\end{equation*}
da cui otteniamo attraverso delle semplificazioni:
\begin{equation*}
    |\lambda| \leq \|A\|
\end{equation*}
Il problema di questa caratterizzazione è che non è facilmente computabile per
$A$ molto grandi.

\begin{definizione} [\textbf{Sfera di Greshgorin}]
    Presa una matrice $A\in \mathbb{R}^{n\times n}$, la \textbf{sfera di Gershgorin} $R_i$ 
    associata all'$i$-esima riga è un cerchio nel piano complesso che ha centro 
    in $a_{ii}$ e raggio $r$.
    $$r = \sqrt{\sum_{i\ne j}|a_{ij}|}$$
    Oppure la sfera si può definire così $$R_i:=\{z\in \mathbb{C}|
    |z-A_{ii}| \leq \sum_{l=1, l\neq j}^{n} |A_{il}|\}$$
\end{definizione}

La sfera di Greshgorin contiene tutti i numeri complessi presenti nel raggio. 

\begin{teorema} [\textbf{Greshgorin}]
    Definiamo $\sigma(A):=\{\lambda | \lambda\text{ è autovalore di }A\}$, esso
    rappresenta lo spettro della matrice. Inoltre, definiamo $R_j:=\{z\in \mathbb{C}|
        |z-A_{jj}| \leq \sum_{l=1, l\neq j}^{n} |A_{jl}|\}$.
    Allora:
    \begin{equation*}
        \sigma(A) \subseteq \bigcup_{j=1}^{n} R_j
    \end{equation*}
    \begin{proof}
        Sia $\lambda = A_{jj}$ per qualche $j = 1 \dots n$ allora $\lambda$ è
        all'interno della sfera.

        Quindi, senza perdere generalità, sia $\lambda \neq A_{jj}, \forall j= 1 \dots n$.
        Sia $\vec{v}_\lambda$ il vettore associato a $\lambda$, conosco che
        $A \vec{v}_\lambda = \lambda \vec{v}_\lambda \implies (A - \lambda Id)
            \vec{v}_\lambda = \vec{0}$. A questo punto possiamo spezzare la matrice 
        $A$ nel seguente modo: $A = D + E$ dove $D = diag(A)$ mentre $E = A - diag(A)$. 
        Sappiamo che $D - \lambda Id$ è una matrice diagonale ed invertibile 
        perché stiamo assumendo che $\lambda \neq A_{jj}, \forall j = 1 \dots n$ (determinante non nullo).
        Quindi possiamo scrivere:
        \begin{equation*}
            (A - \lambda Id)\vec{v}_\lambda = \vec{0} \implies (D + E - \lambda 
            Id)\vec{v}_\lambda = \vec{0} \implies (D - \lambda Id)\vec{v}_\lambda 
            = - E\vec{v}_\lambda \implies \vec{v}_\lambda = -(D - \lambda Id)^{-1}
            E\vec{v}_\lambda
        \end{equation*}
        Quindi possiamo passare alle norme:
        \begin{equation*}
            \|\vec{v}_\lambda\| = \|-(D - \lambda Id)^{-1}E\vec{v}_\lambda\|
        \end{equation*}
        Per ogni norma possiamo dire che:
        \begin{equation*}
            \|\vec{v}_\lambda\| = \|(D - \lambda Id)^{-1}E\vec{v}_\lambda\| \leq
            \|(D - \lambda Id)^{-1}E\| \|\vec{v}_\lambda\|
        \end{equation*}
        Facciamo una digressione, scelgo la norma $\infty$, allora:
        \begin{equation*}
            \|M\vec{x}\|_\infty = \max_{j = 1}^n \sum_{l = 1}^{n}|M_{jl}x_j| \leq
             \max_{j = 1}^n\sum_{l = 1}^{n}|M_{jl}||x_j|\leq \max_{j = 1}^n
             \sum_{l = 1}^{n}|M_{jl}|(\max_{j = 1}^n|x_j|)\leq \|\vec{x}\|_\infty
             \max_{j = 1}^n\sum_{l = 1}^{n}|M_{jl}|
        \end{equation*}

        Tornando alla dimostrazione:
        \begin{equation*}
            \begin{aligned}
                1 \leq \|(D - \lambda Id)^{-1}E\|_\infty \implies 1 \leq \max_{j = 1}^n
                \sum_{l = 1,j \neq l}^{n} \frac{|A_{jl}|}{|A_{jj} - \lambda|} = 
                \sum_{l = 1,l \neq k}^{n} \frac{|A_{kl}|}{|A_{kk} - \lambda|}, 
                \text{ per qualche }k = 1 \dots n \\ \implies |A_{kk} - \lambda| \leq 
                \sum_{l = 1,l \neq k}^{n} |A_{kl}| \implies \lambda \in R_k \implies 
                \lambda \in \bigcup_{j = 1}^n R_j, \forall \lambda \implies 
                \sigma(A) \subseteq \bigcup_{j = 1}^n R_j
            \end{aligned}
        \end{equation*}
    \end{proof}
\end{teorema}
Questo mi permette di identificare l'intervallo in cui sono inclusi tutti gli
autovalori. Quindi se l'intervallo non contiene lo $0$ allora tutti gli autovalori
sono $\neq 0$ e quindi il determinante $\ne 0$, quindi la matrice è invertibile.
\begin{nota}
    $R_j$ è l'insieme dei numeri complessi che distano $\le$ la somma delle entrate
    alla riga $j$ esclusa l'entrata $A_{jj}$. Vengono chiamata \textbf{cerchi Greshgorin}.
\end{nota}
\section{Metodo delle potenze}
Algoritmo per calcolare l'autovalore della matrice $A$ di modulo massimo e 
l'autovettore associato.

Data $A\in \mathbb{R}^{n\times n}$, $\{\lambda_j\}^n_{j = 1}$ autovalori tali che:
\begin{equation*}
    |\lambda_1|> |\lambda_2| \ge \dots \ge |\lambda_N|
\end{equation*}
Il problema di questo metodo è che si assume che $|\lambda_1|> |\lambda_2|$,
se fosse $|\lambda_1| = |\lambda_2|$ allora non si avrebbe la convergenza.

Dato $\underline{q}^{(0)}\in \mathbb{R}^n$ generico tale che $\|\underline{q}^{(0)}\|_2=1$.
A questo punto si itera il seguente algoritmo:
\begin{equation*}
    \underline{q}^{(k+1)} = \frac{A\underline{q}^{(k)}}{\|A\underline{q}^{(k)}\|_2}
\end{equation*}
per ottenere l'autovalore $\lambda_1$ possiamo calcolare l'autovalore massimo come:
\begin{equation*}
    \lambda_1^{(k + 1)} = q^{(k+1)T}Aq^{(k+1)}
\end{equation*}
\begin{nota}
    Se $A$ è diagonalizzabile con autovettori $\{\underline{v}_{\lambda_j}\}$, 
    allora gli autovettori sono una base di $\mathbb{R}^n$.
\end{nota}

Convergenza del metodo delle potenze.
Abbiamo che $v_{\lambda_1}$ è autovettore associato a $\lambda_1$
$$q^{(k)} = \frac{z^{(k)}}{\|z^{(k)}\|_2} = \frac{Aq^{(k-1)}}{\|Aq^{(k-1)}\|_2} =  \frac{A\frac{z^{(k-1)}}{\|z^{(k-1)}\|_2}}{\|A\frac{z^{(k-1)}}{\|z^{(k-1)}\|_2}\|_2} = \frac{Az^{(k-1)}}{\|Az^{(k-1)}\|_2} = \dots = \frac{A^kq^{(0)}}{\|A^kq^{(0)}\|_2}$$

Sappiamo che 
$$A^kq^{(0)} = A^k \cdot (\alpha_1v_{\lambda_1}+\alpha_2v_{\lambda_2} + \dots + \alpha_nv_{\lambda_n}) = \alpha$$

Possiamo utilizzare lo stesso approccio ma per trovare l'autovettore minimo utilizzando 
$A^{-1}$.

\section{Pagerank}
Introduciamo "vicinity matrix" $H\in \mathbb{R}^{n\times n}$ 
tale che 
$$H_{ij} = \begin{cases}
    1 & i\rightarrow j\\
    0 & altrimenti
\end{cases}$$
Introduco il vettore $w$ che contiene in $w_j$ il valore della pagina $j$ esima. 
Insostanza è il vettore di incognita.

Introduciamo anche un vettore $d$, tale che $d_i:=\sum_{l=1\dots n} H_{il}$ $d_i$ 
rappresenta gli archi uscenti.

L'algoritmo di Brin-Page
$$w_j = \sum_{i=1}^n w_i \frac{H_{ij}}{d_i}$$

Il valore della pagina $j$ è la somma dei link entranti pesato per il valore della 
pagina entrante, diviso i link uscenti di $i$.

Per inizializzare il vettore $d = H \cdot e $ dove 
$e = \begin{array}{c} 1 \\ 1 \\ \vdots \\ 1 \end{array}$

Possiamo creare $D = diag(d)$ e definire $M= D^{-1}H$
quindi possiamo dire
$$w_j = \sum_{i=1}^n w_i \frac{H_{ij}}{d_i} \iff w^t M = w \iff M^t w=w$$
Quindi questo è un problema di autovalori e autovettori con autovalore fissato $\lambda = 1$.

Ho diversi problemi in questa modellazione:
\begin{itemize}
    \item se ho $d_i=0$ ovvero una pagina che viene citata ma non cita nessun'altra dangling nodes.
    \item in generale non è detto che esista l'autovalore $1$ per la matrice $M$,
    inoltre se non ha $1$, posso trovare degli autovalori.
    \item se esiste la soluzione è unica?
    \item $w$ ha entrate solo positive? (devono essere positivi i punteggi)
    \item come si calcola $w$?
\end{itemize}
La prima problematica si risolve facendo si che la pagina posso citi tutti Ovvero
che $H_{ij} = \left[0,0,\dots, 0\right] \rightarrow \hat{H}_{ij} =\left[1,1,\dots, 1\right], \forall j$.
Quindi a livello pratico sarà $\hat{H} =H + u \cdot e, e= \left[\begin{array}{c}
    1\\1\\\vdots\\1
\end{array}\right], u_j= \begin{cases}
    1 & j \text{ è un nodo pozzo}\\
    0& altrimenti
\end{cases}$.
Quindi di conseguenza definiremo $\hat{d} = \hat{H}e, \hat{D}=diag(\hat{d}), \hat{M}  =\hat{D}^{-1}\hat{H}$.
Ho sostituito tutte le entry di $H$ con $1$ quindi $D$ è sicuramente invertibile.

Per risolvere i problemi $2,3,4$ si usa il seguente teorema.

\begin{teorema} [\textbf{Frobenius-Perron}]
    Sia $A\in \mathbb{R}^{n\times n}$ con entrate non negative. Allora $\exists \lambda>0$
    autovalore, il cui autovettore associato $v_\lambda$ ha entrate non negative. 
    Inoltre, se $A$ ha entrate positive e $A$ è irriducibile allora 
    $\exists !$ autovalore di modulo massimo $\lambda$, tale che $v_lambda$ ha entrate 
    tutte positive.
\end{teorema}

\begin{definizione}
    $A$ è irriducibile se il mio vicinity graph è fortemente connesso
\end{definizione}
Il problema è che non abbiamo la certezza di avere un vicinity graph fortemente 
connesso. Possiamo modificare l'algoritmo  andando a modificare la matrice $\hat{M}$
con $A= \gamma \hat{M}+ (1-\gamma) e \cdot v^t$ dove $v$ è un vettore di doping 
dove $\sum_{j=1}^n v_j = 1, 0<v_j <1$ e $\gamma \in (0,1)$.

\begin{nota}
    La matrice $A$ ha entrate positive perché $\hat{M}$ ha entrate non negative e 
    $e\cdot v$ ha entrate positive. 
\end{nota}
\begin{nota}
    Se $\gamma = 0$ allora mi dice che le pagine sono tutte uguali perché $w_j = 1$

    Se $\gamma = 1$ allora si ritorna al metodo iniziale che non rispetta il teorema. 
\end{nota}

$e\cdot v$ è un bias che permette di personalizzare l'algoritmo di pagerank.

La conseguenza della prima osservazione porta al fatto che il teorema si può applicare, 
esiste ed è unico l'autovalore di modulo massimo.

L'obiettivo finale è dimostrare che $1$ è l'autovalore di modulo massimo.
\begin{proof}
    Sia $$\|x\|_1 = \sum_{j=1}^n |x_j|$$ 
    $$\|Ax\|_1 = \sum_{j=1}^n |(Ax)_j| = \sum_{j=1}^n|\sum_{l=1}^nA_{jl}x_l|\le \sum_{j=1}^n\sum_{l=1}^n|A_{jl}||x_l| =  \sum_{l=1}^n|x_l|(\sum_{j=1}^n|A_{jl}|)$$

    $$\le  \sum_{l=1}^n|x_l|(\max_{l=1}^n\sum_{j=1}^n|A_{jl}|) = (\max_{l=1}^n\sum_{j=1}^n|A_{jl}|) \sum_{l=1}^n|x_l| = (\max_{l=1}^n\sum_{j=1}^n|A_{jl}|) \|x\|_1 $$

    Quindi $$\|A\|_1 = \sup_{x\in \mathbb{R}^n - \{0\}} \frac{\|Ax\|_1}{\|x\|}\le \max_{l=1}^n\sum_{j=1}^n|A_{jl}|$$

    Inoltre 
    $$|\lambda_{max}| \le \|A\|_1 = \|\gamma \hat{M} + (1-\gamma) e \cdot v^t\|_1 \le \gamma \|\hat{M}^t\|_1 + (1-\gamma) \|e \cdot v^t\|_1$$
    $\|e \cdot v^t\|_1\le 1$  mentre $\|\hat{M}^t\|_1\le 1$, quindi $\le \gamma 1 + (1-\gamma) 1$.

\end{proof}

Quindi abiamo che:
\begin{itemize}
    \item $\|\lambda_{max} | \le 1$
    \item $\lambda = 1$ è autovalore per costruzione 
    \item il teorema implkica esiste un unico autovalore di modulo massimo.
\end{itemize}

