\chapter{Analisi Numerica}
\section{Introduzione}
In questo capitolo verranno presentate alcune tecniche per la risoluzione dei
sistemi lineari e il concetto di \textbf{numero di condizionamento}.

Lo sviluppo di un algoritmo di risoluzione di sistemi lineari deve considerare
tre fattori:
\begin{enumerate}
    \item \textbf{Memoria}: l'algoritmo deve richiedere un quantitativo ridotto
          di memoria.
    \item \textbf{Velocità}: l'algoritmo deve essere molto veloce.
    \item \textbf{Precisione}: vogliamo ottenere una soluzione più vicina possibile
          a quella reale.
\end{enumerate}
\subsection{Gestione di Matrici e Vettori}
A livello di programmazione, i vettori possono essere salvati utilizzando
aree congiunte di memoria (array). Mentre, per quanto riguarda le matrici, si
possono salvare in array secondo diverse filosofie:
\begin{itemize}
    \item \textbf{Row Major}: si salvano le righe una dopo l'altra.
          \begin{figure}[!ht]
              \centering
              \includegraphics[width=0.25\textwidth]{./img/AnalisiNumerica/RowMajor.png}
              \caption{Esempio di salvataggio di una matrice in Row Major}
          \end{figure}
    \item \textbf{Column Major}: si salvano le colonne una dopo l'altra.
          \begin{figure}[!ht]
              \centering
              \includegraphics[width=0.25\textwidth]{./img/AnalisiNumerica/ColumnMajor.png}
              \caption{Esempio di salvataggio di una matrice in Column Major}
          \end{figure}
\end{itemize}

Per salvare una matrice \textbf{sparsa} viene sempre salvata come collezione di
tuple $\langle i, \, j,\, val\rangle$. Con questa rappresentazione, anche nota
come \textbf{formato sparso}, ogni qualvolta si effettua un'operazione tra matrici
sparse si trasformano in matrici normali e si applica l'operazione.
\subsection{Gestione Errori}
L'algoritmo creato deve essere valutato in base a quanto è vicino il risultato
restituito rispetto a quello corretto. Questo deve essere fatto in quanto
i risultati sono spesso forniti come numeri floating point e quindi soggetti
a errori di approssimazione.

\begin{definizione}[\textbf{Errore assoluto}]
    Sia $\stackrel{\sim}{\textbf{x}}$ la soluzione approssimata di un sistema
    lineare $A\textbf{x} = \textbf{b}$ e sia $x$ la soluzione corretta, allora
    l'\textbf{errore assoluto} è definito come:
    \begin{equation}
        \epsilon = \|\stackrel{\sim}{\textbf{x}} - \textbf{x}\|
    \end{equation}
    dove $\|\cdot\|$ è una qualsiasi norma fra due vettori.
\end{definizione}
\begin{definizione}[\textbf{Errore relativo}]
    Sia $\stackrel{\sim}{\textbf{x}}$ la soluzione approssimata di un sistema
    lineare $A\textbf{x} = \textbf{b}$ e sia $x$ la soluzione corretta, allora
    l'\textbf{errore relativo} è definito come:
    \begin{equation}
        \epsilon = \frac{\|\stackrel{\sim}{\textbf{x}} - \textbf{x}\|}{\|\textbf{x}\|}
    \end{equation}
    dove $\|\cdot\|$ è una qualsiasi norma fra due vettori.
\end{definizione}

Il problema è conoscere la soluzione esatta del sistema per calcolare l'errore.
Un modo per ottenere la soluzione esatta possiamo inizializare un vettore $x$ in modo
randomico. In questo modo si calcola il vettore $b =A\cdot x $ e successivamente
si utilizza algoritmo di risoluzione di sistemi per risolvere il seguente
sistema $A\cdot \stackrel{\sim}{x} = b$. In questo modo $x$ è la soluzione esatta
mentre  $\stackrel{\sim}{x}$ è la soluzione approssimata. Ovviamente questa non è
la soluzione reale, ma ci interessa principalmente sapere se è risolvibile il sistema
e con che errore. Questo è determinato dalla matrice $A$ ma non dal vettore dei
termini noti $b$, per questo ci si calcola un vettore $b$ dipendente dalla soluzione
"esatta" randomica.

\begin{teorema}\label{teo:NormeEquivalenti}
    Il comportamento delle norme di vettori in $\mathbb{R}^n$ è equivalente. ossia
    preso un vettore $x\in \mathbb{R}^n$, $\exists c_1,c_2\in \mathbb{R}$ tali che:
    \begin{equation}
        c_1\|x\|_p < \|x\|_q < c_2\|x\|_p
    \end{equation}
\end{teorema}
Il teorema \ref{teo:NormeEquivalenti} ci dice che le norme a meno di alcuni
fattori moltiplicativi restituiscono gli stessi risultati.
\subsection{Velocità}
Come precedentemente detto, un aspetto importante è la velocità dell'algoritmo.
Per calcolare questo parametro dobbiamo contare il numero di operazioni che
effettuiamo, nello specifico si andrà ad utilizzare la complessità asintotica.

Analizziamo ora le velocità delle diverse operazioni:
\begin{itemize}
    \item \textbf{Prodotto Scalare}: sia $(x,y) = \sum_{i=1}^n x_iy_i$ corrisponde
          ad effettuare $n$ prodotti e $n-1$ somme, quindi si ha $n + n - 1 = 2
              n -1 \in \mathcal{O}(n)$
    \item \textbf{Norma di un vettore}: sia $\|x\| = \sqrt{\sum_{i=1}^n x_i^2}$
          corrisponde ad effettuare $n$ prodotti e $n-1$ somme, quindi si ha
          $n + n - 1 = 2n -1 \in \mathcal{O}(n)$
    \item \textbf{Prodotto Matrice-Vettore}: siano $A\in \mathbb{R}^{r\times c}$
          e $v\in \mathbb{R}^c$ allora $Av\in \mathcal{O} (rc) = \mathcal{O}(n^2)$
    \item \textbf{prodotto Matrice-Matrice}: siano $A,B\in \mathbb{R}^{n\times n}$
          allora $AB\in \mathcal{O} (n \cdot n \cdot n) = \mathcal{O} (n^3)$
    \item \textbf{Determinate}: sia $A\in \mathbb{R}^{n\times n}$ allora
          $det(A) \in \mathcal{O}(n!)$.
\end{itemize}
\section{Numero di Condizionamento}
Sappiamo che presa una matrice $A \in \mathbb{R}^{n \times n}$ e un vettore $b
    \in \mathbb{R}^n$ diverso dal vettore nullo, il sistema lineare $A \textbf{x} =
    \textbf{b}$ ammette soluzione se e soltanto se $det(A) \neq 0$.

Purtroppo quando si risolvono i sistemi lineari al calcolatore subentrano una
serie di altri fattori che influenzano il modo in cui si risolve il sistema e,
di conseguenza, la soluzione stessa. Questi fattori sono dovuti essenzialmente
alla rappresentazione dei numeri e agli errori di arrotondamento nelle operazioni.

Oltre a ciò, analizzando le velocità delle operazioni, si può notare che il
calcolo del determinante è molto complesso, quindi non è possibile utilizzarlo
per capire se un sistema è complesso da risolvere.

Per ovviare a questo problema, si introduce il concetto di \textbf{numero di
    condizionamento} calcolato su una matrice. Se tale valore è alto allora il
sistema è complesso da risolvere.
\begin{definizione}[\textbf{Numero di Condizionamento}]
    Presa una matrice $A \in \mathbb{R}^{n \times n}$, il \textbf{numero di
        condizionamento} è definito come:
    \begin{equation}
        cond(A) = \|\lambda_{max}\| \cdot \|\lambda_{min}\|
    \end{equation}
    dove $\lambda_{max}$ e $\lambda_{min}$ sono rispettivamente il massimo e il
    minimo autovalore della matrice $A$ in modulo.
\end{definizione}
Per definizione il numero di condizionamento è una quantità maggiore di $1$.
Vedremo che più questa quantità è vicino a $1$ più il sistema sarà facile da
risolvere numericamente, ossia risente meno dell'algebra di macchina. Di contro
più grande è questa quantità più il sistema sarà risolto “male”.
\begin{esempio}[Numero di Condizionamento di una Matrice di Hillbert]
    Prendiamo una matrice $H \in \mathbb{R}^{n \times n}$ definita nel seguente modo:
    \begin{equation*}
        H_{ij}=\frac{1}{1+i+j}
    \end{equation*}
    Questa matrice è nota con il nome di matrice di Hillbert ed è il classico
    esempio di matrice mal condizionata.
\end{esempio}
\section{Metodi per risolvere i sistemi}
Esistono due tipologie di metodi per la risoluzione dei sistemi lineari:
\begin{itemize}
    \item \textbf{Metodi diretti}: si effettuano delle operazioni per ottenere
          in automatico la soluzione dei sistemi.
    \item \textbf{Metodi iterativi}: sono procedure che creano una sequenza di
          vettori soluzioni che si avvicinano sempre di più alla soluzione esatta
          $\|x -\stackrel{\sim}{x}\|\rightarrow 0$ oppure si calcola $\frac{\|x
                  -\stackrel{\sim}{x}\|}{\|A\stackrel{\sim}{x} - b\|}\rightarrow 0$.
\end{itemize}

\section{Metodi diretti}
Partiamo analizzando il caso più semplice, ovvero che il sistema ammette un unica
soluzione. L'assunzione principale è che le matrici abbiano determinante diverso
da $0$.
\subsection{Sostituzione all'indietro}
Il metodo della \textbf{sostituzione all'indietro} è il metodo più efficiente per
risolvere sistemi composti da una matrice quadrata triangolare superiore.
\begin{equation}
    \begin{cases}
        u_{11}x_1 + u_{12} x_2 + \dots + u_{1n} x_n= b_1 \\
        u_{22}x_2 + u_{23} x_3 + \dots + u_{2n} x_n= b_2 \\
        \vdots                                           \\
        u_{nn}x_n = b_n                                  \\
    \end{cases} \iff \left[\begin{array}{cccc}
            u_{11} & u_{12} & \dots  & u_{1n} \\
            0      & u_{22} & \dots  & u_{2n} \\
            \vdots & \ddots & \ddots & \vdots \\
            0      & \dots  & 0      & u_{nn}
        \end{array} \right] \cdot x = \left[\begin{array}{c}
            b_1    \\
            b_2    \\
            \vdots \\
            b_n
        \end{array}\right]
\end{equation}
A livello computazione è semplice testare se la matrice è compatibile con questo
metodo.

Una volta effettuata la verifica, il metodo coincide con il risolvere il sistema
per sostituzione.
\begin{nota}
    Bisogna stare attenti che la diagonale non contenga $0$ perché ogni volta
    si divide per un termine sulla diagonale.
\end{nota}

Sotto le ipotesi di matrice triangolare superiore possiamo calcolare facilmente
il determinante, basta calcolare il prodotto dei termini sulla diagonale, questo
ci dice che esiste una sola soluzione se il determinante è diverso da $0$
($\mathcal{O}(n)$). In aggiunta, questo ci assicura che la diagonale non contiene $0$.

La complessità dell'algoritmo sarà:
\begin{equation*}
    \#\_righe \cdot \#operazioni\_max\_riga = n \cdot (n - 1 + n - 1 + 1) = n \cdot (2n - 1) = \mathcal{O}(n^2)
\end{equation*}
\begin{nota}
    Dove il numero massimo di operazioni per riga è $n - 1$ differenze, $n - 1$
    prodotti e $1$ divisione perché nella prima riga si ha:
    \begin{equation*}
        x_1 = \frac{(b_1 - u_{12} x_2 - u_{13}x_3 - \dots - x_{1n} x_n)}{u_{11}}
    \end{equation*}
\end{nota}

Ogni singola operazione per riga si può ricavare iterativamente nel seguente modo
\begin{equation*}
    x_{n-i} = \frac{(b_i-(u_{n-i},x))}{u_{ii}}
\end{equation*}
Con $x$ inizializzato a tutti $0$ tranne la cella $n$ che sarà inizializzata a:
\begin{equation*}
    x_n = \frac{b_n}{u_{nn}}
\end{equation*}
Mentre $i$ è inizializzato a $1$, ad ogni iterazione si utilizza il vettore $x$
aggiornato nell'iterazione precedente. Al termine si ottiene una soluzione per il
sistema.
\begin{nota}
    Questo metodo può essere utilizzato anche per i sistemi lineari descritti da
    una matrice triangolare inferiore. In questo caso prima sarà necessario
    trasformare la matrice in una matrice triangolare superiore. Questo si può
    fare riordinando le righe della matrice e specchiando la matrice la colonna
    centrale. Lo stesso ragionamento si può applicare anche per matrici diagonali
    rispetto alla antidiagonale.
\end{nota}
\begin{nota}
    Ricorda che quando si effettua un permutazione delle righe il sistema rimane
    invariato, al contrario se effettuassi una permutazione delle colonne allora 
    sto cambiando le incognite, perciò dovrei cambiare anche il vettore delle incognite
    se io volessi mantenere lo stesso sistema.
\end{nota}
\subsection{Trasformazioni di Gau$\Beta$}
Per risolvere matrici di questo tipo non possiamo più pensare all'algoritmo precedente.
\begin{equation}
    \begin{cases}
        a_{11}x_1 + a_{12} x_2 + \dots + a_{1n} x_n= b_1 \\
        a_{21}x_1 + a_{22} x_2 + \dots + a_{2n} x_n= b_2 \\
        \vdots                                           \\
        a_{n1}x_1 + a_{n2} x_2 + \dots + a_{nn} x_n= b_n \\
    \end{cases} 
\end{equation}

Per matrici generiche i metodi variano, si può pensare di trasformare la matrice
in una triangolare superiore equivalente, utilizzando le trasformazioni di Gauss:
\begin{itemize}
    \item \textbf{Scambiare le righe}
    \item \textbf{Riduzione}: Sostituire una riga con una combinazione lineare di se stessa
              con un'altra
\end{itemize}
Successivamente si eseguerà sul sistema equivalente il metodo di risoluzione per 
sostituzione all'indietro.

Per la riduzione della matrice del sistema ad una triangolare superiore applichiamo 
sempre le trasformazioni in modo da azzerare i coefficienti al di sotto della 
diagonale. L'applicazioni delle trasformazioni deve eessere effettuata sulle righe 
della \textbf{augmented matrix form} che corrisponde alla matrice del sistema con
l'aggiunta del vettore dei termini noti a destra dei coefficienti delle incognite.

\begin{nota}
    Sotto l'ipotesi di sistema risolubile (determinante non nullo) allora il metodo 
    di Gau$\Beta$ convergerà ad una matrice triangolare superiore. Quindi non rischio
    di sostituire una riga con un coefficiente moltiplicatore di un'altra riga 
    avente denominatore nullo. Quindi quando abbiamo una riga che ha il coefficiente 
    sulla diagonale pari a $0$ allora siamo sicuri che troveremo un'altra riga 
    con un coefficiente non nullo.
\end{nota}

Calcoliamo la complessità, il peggiore dei casi è quando effettuiamo la prima riduzione.
Nella prima riduzione dobbiamo calcolare 
$$R_i = R_i - \frac{a_{11}\cdot a_{a_i1}}{a_{11}} R_1$$
Queste operazioni sono $n+1$ differenze, $n+1$ prodotti e  $1$ divisione. Quindi 
per una riga sono $1+n+1+n+1=2n+3=\mathcal{O}(n)$, questo deve essere ripetuto per $n-1$
volte per azzerare la prima colonna, quindi $\mathcal{O}(n^2)$. Dal momento che dobbiamo 
ridurre un totale di $n-1$ colonne quindi si ha $\mathcal{O}(n^2\cdot n) = \mathcal{O}(n^3)$.
Attualmente non stiamo risolvendo il sistema, bensì lo stiamo solo trasformando, 
per anche risolverlo impiegheremo  $\mathcal{O}(n^3+ n^2) =\mathcal{O}(n^3)$.

La scelta su da quale equazione partire porta ad avere cambiamenti sugli errori 
ottenuti dal calcolo della soluzione dovuta all'aritmetica floating point. Quindi 
l'obiettivo sarà di ridurre al minimo l'errore, sapendo che l'operazione problematica 
è la somma/differenza sopprattutto quando effettuiamo il calcolo $x-x$. Quindi 
possiamo mitigare questi problemi utilizzando il \textbf{pivoting}, ovvere scegliere 
come pivot quello in valore assoluto maggiore presente sulla colonna da ridurre.  
Si sceglie il maggiore perché supponendo di essere in questo esempio:
$$a_{21} - \frac{a_{21}}{a_11} \ a_{22}-\frac{a_{21}}{a_11}$$
Sapendo che $a_{21}=0$ allora sostituisco direttamente con $0$ senza effettuare 
la sottrazione (non compiendo errori) e se si sceglie come pivot il massimo allora 
$\frac{a_{21}}{a_11}$ sarà piccolo quindi si compie un errore minore nel calcolo 
di $a_{22}-\frac{a_{21}}{a_11}$. Si può ottimizzare ancora meglio effettuando il 
\textbf{pivoting totale}. 
% TODO: aggiungi la tabella del confronto degli errori utilizzando diversi metodi di pivoting 

Utilizzando il metodo di Gau$\Beta$ ha un problema, quando dobbiamo risolvere i 
seguenti sistemi
$$Ax= b_1 \ \ Ax=b_2 \ \ Ax=b_3$$
Dobbiamo ogni volta riapplicare le trasformazioni. 

Sapendo che i termini noti sono indipendenti dalla selta del pivoting allora dobbiamo
trovare un modo per aggiornare $b_i$ per evitare di aggiornare ogni volta $A$. 
Risolvere più volte questo sistema variando $b_i$ coincide con il calcolo dell'inversa.
Dove $x$ corrisponde alla colonna $i$ dell'inversa con il vettore di termini noti $b_i$,
dove $b_i$ è il vettore di tutti $0$ tranne un $1$ nella posizione $i$-esima.7

Per risolvere questo problema uso la decomposizione LU. Ovvero dato un sistema $Ax=b$,
l'idea è di riscrivere $A=L\cdot U$ che rappresentano rispettivamente una Matrice
triangolare inferiore e una matrice triangolare superiore. Quindi si risolverà 
il sistema $L\cdot U \cdot x =b$, quando dobbiamo cambiare $b$ basta sostituire 
$b$ e le matrici $L$ e $U$ rimarranno uguali.

Per risolvere $L\cdot U \cdot x =b$, prima si risolve $L\cdot y =b$ con una \textbf{
    soluzione in avanti} $\mathcal{O}(n^2)$, poi si risolve $U\cdot x =y$ con una \textbf{
        soluzione all'indietro} $\mathcal{O}(n^2)$.

\begin{teorema}
    Sia $A\in \mathbb{R}^{n\times n}$ una matrice non singolare (determinante 
    non nullo), allora esiste $L,U \in \mathbb{R}^{n\times n}$ talu che $A=L\cdot U$.
    \begin{equation*}
        U=\left[\begin{array}{ccc}
            a_{11} & a_{12} & a_{13}\\
            0 & a_{22}' & a_{23}'\\
            0 & 0 & a_{33}''\\
        \end{array}\right] \ \ L=\left[\begin{array}{ccc}
            1 & 0 & 0\\
            \frac{a_{ 21}}{a_{11}} & 1 & 0\\
            \frac{a_{31}}{a_{11}} & \frac{a_{32}'}{a_{22}'} & 1\\
        \end{array}\right]
    \end{equation*}

\end{teorema}